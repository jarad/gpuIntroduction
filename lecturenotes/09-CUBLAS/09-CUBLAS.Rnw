\documentclass{article}

\hoffset = 0pt
\voffset = 0pt
\footskip = 75pt

\usepackage[landscape]{geometry}
\usepackage{amssymb,amsfonts}
\usepackage[all,arc]{xy}
\usepackage{enumerate}
\usepackage{mathrsfs}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{mathrsfs}
\usepackage[all,arc]{xy}
\usepackage{enumerate}
\usepackage{color}
\usepackage{graphicx}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{subfig}
\usepackage{color}
\usepackage{Sweave}
\usepackage{SASnRdisplay}

\providecommand{\beq}{\begin{equation*}}
\providecommand{\eeq}{\end{equation*}}
\providecommand{\bs}{\backslash}
\providecommand{\e}{\varepsilon}
\providecommand{\E}{\ \exists \ }
\providecommand{\all}{\ \forall \ }
\providecommand{\Rt}{\Rightarrow}
\providecommand{\rt}{\rightarrow}
\providecommand{\vc}[1]{\boldsymbol{#1}}
\providecommand{\N}{\mathbb{N}}
\providecommand{\Q}{\mathbb{Q}}
\providecommand{\R}{\mathbb{R}}
\providecommand{\C}{\mathbb{C}}
\providecommand{\Z}{\mathbb{Z}}
\providecommand{\Qn}{\mathbb{Q}^n}
\providecommand{\Rn}{\mathbb{R}^n}
\providecommand{\Cn}{\mathbb{C}^n}
\providecommand{\Zn}{\mathbb{Z}^n}
\providecommand{\Qk}{\mathbb{Q}^k}
\providecommand{\Rk}{\mathbb{R}^k}
\providecommand{\Ck}{\mathbb{C}^k}
\providecommand{\Zk}{\mathbb{Z}^k}
\providecommand{\ov}[1]{\overline{#1}}
\providecommand{\lmu}[1]{\lim_{#1 \rightarrow \infty}}
\providecommand{\lmd}[1]{\lim_{#1 \rightarrow -\infty}}
\providecommand{\lm}[2]{\lim_{#1 \rightarrow #2}}
\providecommand{\nv}{{}^{-1}}
\providecommand{\aut}[1]{\text{Aut}{ \ #1}}
\providecommand{\inn}[1]{\text{Inn}{ \ #1}}
\providecommand{\cj}[1]{\overline{#1}}
\providecommand{\wh}[1]{\widehat{#1}}

\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{quest}[thm]{Question}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{defns}[thm]{Definitions}
\newtheorem{con}[thm]{Construction}
\newtheorem{exmp}[thm]{Example}
\newtheorem{exmps}[thm]{Examples}
\newtheorem{notn}[thm]{Notation}

\newtheorem{notns}[thm]{Notations}
\newtheorem{addm}[thm]{Addendum}
\newtheorem{exer}[thm]{Exercise}

\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem{rems}[thm]{Remarks}
\newtheorem{warn}[thm]{Warning}
\newtheorem{sch}[thm]{Scholium}

\makeatletter
\let\c@equation\c@thm
\makeatother
\numberwithin{equation}{section}

\bibliographystyle{plain}

\providecommand{\pset}{1}   %PUT PROBLEM SET NUMBRER HERE
\renewcommand{\labelenumi}{\alph{enumi}.} %controls enumerating style
\renewcommand{\labelenumii}{\roman{enumii}.} 

\setcounter{section}{\pset}

\begin{document}
\begin{flushleft}

\Huge
\begin{center}
$\quad$ \newline
$\quad$ \newline
$\quad$ \newline
$\quad$ \newline
{\bf THE CUBLAS LIBRARY}
\end{center} $\quad$ \newline

\LARGE

\begin{center}
Will Landau, Prof. Jarad Niemi
\end{center}

\newpage

\Huge
\begin{center}
{\bf WHAT IS CUBLAS?}
\end{center} $\quad$ \newline \huge

CUBLAS library is a CUDA C implementation of the C/Fortran library, BLAS (Basic Linear Algebra Subprograms). \newline

3 ``levels of functionality": \newline

\begin{enumerate}[Level 1: ]
\item ${\bf y} \mapsto \alpha {\bf x} + {\bf y} \qquad \quad \ \ \ $ and other vector-vector routines
\item ${\bf y} \mapsto \alpha A {\bf x} + {\bf \beta y} \qquad \ \ $ and other vector-matrix routines
\item $C \mapsto \alpha AB + \beta C \qquad$ and other matrix-matrix routines
\end{enumerate} $\quad$ \newline

where $\alpha$ and $\beta$ are scalars, ${\bf x}$ and ${\bf y}$ are vectors, and $A$, $B$, and $C$ are matrices.

\newpage


\Huge
\begin{center}
{\bf BEFORE COMPILING WITH CULBAS, CHOOSE WHICH .h FILE TO USE}
\end{center} $\quad$ \newline \Large

CUBLAS version 4.0 and above has a different API, which is supposed to be better. \newline

Include ``cublas\_v2.h", for the new API. Use this one for new programs. \newline
Include ``cublas.h" for the old API. Use this one for programs that depend on the old API. \newline

Things on the new API but not the old:

\begin{itemize}
\item {\tt cublasCreate} initializes the handle to the CUBLAS library context, allowing more user control. 
\item Scalars $\alpha$ and $\beta$ can be passed by reference to host and device functions in addition to by value.
\item Scalars can be returned by reference in addition to by value.
\item All CUBLAS functions return an error status, {\tt cublasStatus\_t}.  
\item {\tt cublasAlloc()} and {\tt cublasFree()} are deprecated. Use {\tt cudaMalloc()} and {\tt cudaFree()} instead.
\item {\tt cublasSetKernelStream()} was renamed {\tt cublasSetStream()}.
\end{itemize}

\newpage

\Huge
\begin{center}
{\bf COMPILING WITH CUBLAS}
\end{center} $\quad$ \newline \LARGE

\begin{enumerate}[1. ]
\item Include either ``cublas\_v2.h" or ``cublas.h" in your source.
\item Compile with: {\tt \bf mvcc -lcublas your\_source.cu -o your\_binary}
\end{enumerate} $\quad$ \newline
Example: \newline

\setkeys{Gin}{width=.7\textwidth} \includegraphics[scale=0.25,angle=0]{compile} \newline

Then I can run the binary: \newline

\setkeys{Gin}{width=.5\textwidth} \includegraphics[scale=0.25,angle=0]{run} 


\newpage

\Huge
\begin{center}
{\bf IMPLEMENTATION OF MATRICES}
\end{center} $\quad$ \newline \Large

Matrices are implemented as linear arrays of memory. For example, CUBLAS thinks of this memory array: \newline

\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
1 & 1 & 2 & 3 & 5 & 8 & 13 & 21 & 34 & 55 & 89 & 144 \\ \hline
\end{tabular} $\quad$ \newline

as this matrix: \newline

\begin{align*}
\begin{bmatrix}
1 & 2 & 5 & 13 & 34 & 89 \\ 
1 & 3 & 8 & 21 & 55 & 144 \\ 
\end{bmatrix} \qquad \text{or this matrix:} \qquad
\begin{bmatrix}
1 & 5 & 34 \\
1 & 8 & 55 \\
2 & 13 & 89 \\
3 & 21 & 144
\end{bmatrix}
\end{align*}

$\quad$ \newline

depending on the number of rows and columns you specify. \newline

NOTE: CUBLAS indexes matrices in column major format.

\newpage

Let: \newline

 $A = $ \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
1 & 1 & 2 & 3 & 5 & 8 & 13 & 21 & 34 & 55 & 89 & 144 \\ \hline
\end{tabular} $\quad$ \newline
\begin{align*}
B = \begin{bmatrix}
1 & 5 & 34 \\
1 & 8 & 55 \\
2 & 13 & 89 \\
3 & 21 & 144
\end{bmatrix}
\end{align*}

Then:

\begin{align*}
B[\text{row }i, \text{ col }j] = A[j \cdot ld + i]
\end{align*} $\quad$ \newline

Where $ld$ stands for ``lead dimension". For column major order matrices, the lead dimension of a matrix is the number of elements in a column. $\quad$ \newline

For indexing in your code, use a function or macro such as:

\begin{verbatim}
                       #define IDX2F(i, j, ld) j * ld + i 
\end{verbatim} $\quad$ \newline

To go from matrix coordinates to the corresponding memory array index. \footnote{Note: use {\tt \#define IDX2F(i, j, ld) (j - 1) * ld + i-1 } for 1-bases matrix coordinates}



\newpage

\Huge
\begin{center}
{\bf  CUBLAS CONTEXT}
\end{center}  \huge

For CUBLAS version 4.0 and beyond, you must wrap your code like this:

\begin{verbatim}
    cublasHandle_t handle;
    cublasCreate(&handle);

    // your code

    cublasDestroy(handle);
\end{verbatim} $\quad$ \newline

and pass {\tt handle} to every CUBLAS function in your code. \newline

This approach allows the user to use multiple host threads and multiple GPUs.  \newpage

\Huge
\begin{center}
{\bf STREAMS}
\end{center} $\quad$ \newline

Streams provide a way to run multiple \emph{ kernels} simultaneously on the GPU. \newline

For more information, look up the following functions: \newline

{\tt cudaStreamCreate()} \newline
{\tt cublasSetStream()}
 \newpage

\Huge
\begin{center}
{\bf CUBLAS HELPER FUNCTIONS}
\end{center} $\quad$ \newline \Large


\begin{verbatim}
cublasSetVector()
cublasGetVector()
cublasSetMatrix()
cublasGetMatrix()
\end{verbatim} $\quad$ \newline


\newpage

\begin{verbatim}
cublasStatus_t cublasSetVector(int n, int elemSize,
                               const void *x, int incx, void *devicePtr, int incy)
\end{verbatim} $\quad$ \newline

Copies a CPU vector {\tt x} to a GPU vector {\tt y} pointed to by {\tt devicePtr}. \newline

\begin{itemize}
\item {\tt n}: number of elements copied from {\tt x}
\item {\tt elemSize}: size, in bytes, of each element copies
\item {\tt incx}: storage spacing between consecutive elements of CPU vector,  {\tt x}.
\item {\tt incy}: storage spacing between consecutive elements of GPU vector, {\tt y} (or {\tt devicePtr}).
\end{itemize}

\newpage


\begin{verbatim}
cublasStatus_t cublasGetVector(int n, int elemSize,
                               const void *x, int incx, void *y, int incy)
\end{verbatim} $\quad$ \newline

Copies a GPU vector {\tt x} to a CPU vector {\tt y} pointed to by {\tt devicePtr}. \newline

\begin{itemize}
\item {\tt n}: number of elements copied from {\tt x}
\item {\tt elemSize}: size, in bytes, of each element copies
\item {\tt incx}: storage spacing between consecutive elements of CPU vector,  {\tt x}.
\item {\tt incy}: storage spacing between consecutive elements of GPU vector, {\tt y} (or {\tt devicePtr}).
\end{itemize}

\newpage


\begin{verbatim}
cublasStatus_t cublasSetMatrix(int rows, int cols, int elemSize,
                               const void *A, int lda, void *B, int ldb)
\end{verbatim} $\quad$ \newline

Copies a column-major CPU matrix A to a column-major GPU matrix B. \newline \newline \newline

\begin{verbatim}
cublasStatus_t cublasGetMatrix(int rows, int cols, int elemSize,
                               const void *A, int lda, void *B, int ldb)
\end{verbatim} $\quad$ \newline

Copies a column-major GPU matrix A to a column-major CPU matrix B. \newline \newline


\begin{itemize}
\item {\tt lda}: number of rows in {\tt A}
\item {\tt ldv}: number of rows in {\tt B}
\end{itemize} $\quad$ \newline
\newpage


\Huge
\begin{center}
{\bf LEVEL 1 FUNCTIONS}
\end{center}  \Large $\quad$ \newline

\begin{tabular}{|p{4.5cm}|p{3.75cm}|p{3.75cm}|p{3.75cm}|p{4.75cm}|}
\hline
{\bf In R:} & {\bf float} & {\bf double} & {\bf cuComplex} & {\bf cuDoubleComplex} \\ \hline
which.max({\bf x}) & {\tt cublasIsamax()} & {\tt cublasIdamax()}  & {\tt cublasIcamax()} & {\tt cublasIzamax()}  \\ \hline
which.min({\bf x}) & {\tt cublasIsamin()} & {\tt cublasIdamin()}  & {\tt cublasIcamin()} & {\tt cublasIzamin()}  \\ \hline
sum(abs({\bf x})) & {\tt cublasSasum()} & {\tt cublasDasum()}  & {\tt cublasScasum()}  & {\tt cublasDzasum()} \\ \hline
{\tt $\alpha$*{\bf x} + {\bf y} -> {\bf y}} & {\tt cublasSaxpy()} &{\tt cublasDaxpy()} &{\tt cublasCaxpy()} & {\tt cublasZaxpy()}  \\ \hline
{\tt {\bf x} -> {\bf y}}& {\tt cublasScopy()} & {\tt cublasDcopy()} & {\tt cublasCcopy()} & {\tt cublasZcopy()}  \\ \hline
{\tt sum({\bf x} * {\bf y})}& {\tt cublasSdot()} & {\tt cublasDdot()}  & {\tt cublasCdotu()} {\tt cublasCdotc()} & {\tt cublasZdotu()} {\tt cublasZdotc()} \\ \hline
{\tt sqrt(sum({\bf x}${}^2$))} & {\tt cublasSnrm2()} & {\tt cublasDnrm2()} & {\tt cublasScnrm2()} & {\tt cublasDznrm2()}\\ \hline
{\tt G \%*\% {\bf x}; G \%*\% {\bf y}} & {\tt cublasSrot()} & {\tt cublasDrot()} & {\tt cublasCrot()}  {\tt cublasCsrot()} & {\tt cublasZrot()} {\tt cublasZdrot()}  \\ \hline
 {\tt H \%*\% {\bf x}; H \%*\% {\bf y}} & {\tt cublasSrotm()} & {\tt cublasDotm()}  & & \\ \hline
{\tt $\alpha$ * {\bf x} -> {\bf x}} & {\tt cublasSscal()} &{\tt cublasDscal()} & {\tt cublasCscal()} {\tt cublasCsscal()} & {\tt cublasZscal()} {\tt cublasZdscal()} \\ \hline
{\bf x} -> {\bf m}; {\bf y} -> {\bf x}; {\bf m} -> {\bf y}& {\tt cublasSswap()} & {\tt cublasDswap()} & {\tt cublasCswap()} & {\tt cublasZswap()} \\ \hline
\end{tabular} $\quad$ \newline \newline

Where $\alpha$ is a scalar, {\bf \tt x} and {\bf \tt y} are vectors, {\tt G} = $\begin{bmatrix} c & s \\ -s & c \end{bmatrix}$, and {\tt H} is some $2 \times 2$ matrix. 


\newpage

\Huge
\begin{center}
{\bf LEVEL 2 FUNCTIONS}
\end{center} \LARGE

\begin{align*}
\text{op}(A) {\bf x} \rt {\bf x}
\end{align*}

where: 
\begin{displaymath}
   \text{op}(A) = \left\{
     \begin{array}{lr}
       A & : \text{{\tt transa == CUBLAS\_OP\_N}}\\
       A^T & : \text{{\tt transa == CUBLAS\_OP\_T}}\\
       A^H & : \text{{\tt transa == CUBLAS\_OP\_C}}
     \end{array}
   \right.
\end{displaymath} $\quad$ \newline



\begin{tabular}{|p{4cm}|c|c|c|c|}
\hline
type of matrix $A$ & float & double & cuComplex & cuDoubleComplex \\ \hline
{\large triangular, banded} & cublasStbmv() & cublasDtbmv() & cublasCtbmv() & cublasZtbmv() \\ \hline
{\large triangular, packed format} & cublasStpmv() & cublasDtpmv() & cublasCtpmv() & cublasZtpmv() \\ \hline
{\large triangular, upper or lower mode} & cublasStrmv() & cublasDtrmv() & cublasCtrmv() & cublasZtrmv() \\ \hline
\end{tabular}

\newpage

\begin{align*}
\text{Solve $\quad$ op}(A) {\bf x} = {\bf b} \text{ $\quad$ for } {\bf x}
\end{align*}
where: 
\begin{displaymath}
   \text{op}(A) = \left\{
     \begin{array}{lr}
       A & : \text{{\tt transa == CUBLAS\_OP\_N}}\\
       A^T & : \text{{\tt transa == CUBLAS\_OP\_T}}\\
       A^H & : \text{{\tt transa == CUBLAS\_OP\_C}}
     \end{array}
   \right.
\end{displaymath} $\quad$ \newline


\begin{tabular}{|c|c|c|c|c|}
\hline
type of matrix $A$ & float & double & cuComplex & cuDoubleComplex \\ \hline
{\large triangular, banded} & cublasStbsv() & cublasDtbsv() & cublasCtbsv() & cublasZtbsv() \\ \hline
{\large triangular, packed format} & cublasStpsv() & cublasDtpsv() & cublasCtpsv() & cublasZtpsv() \\ \hline
{\large triangular, upper or lower mode} & cublasStrsv() & cublasDtrsv() & cublasCtrsv() & cublasZtrsv() \\ \hline
\end{tabular}
\newpage



\begin{align*}
\alpha {\bf x} {\bf x}^T + A \rt A
\end{align*}


\begin{tabular}{|c|c|c|c|c|}
\hline
type of matrix $A$ & float & double \\ \hline
{\large symmetric matrix}& cublasSsyr() & cublasDsyr() \\ \hline
{\large symmetric matrix in packed format }& cublasSspr() & cublasDspr() \\ \hline
\end{tabular} $\quad$ \newline \newline \newline

\begin{align*}
\alpha {\bf x} {\bf x}^H + A \rt A
\end{align*}

\begin{tabular}{|c|c|c|c|c|}
\hline
type of matrix $A$ & cuComplex & cuDoubleComplex \\ \hline
{\large Hermitian}& cublasCher() & cublasZher() \\ \hline
{\large Hermitian, packed format}& cublasChpr() & cublasZhpr() \\ \hline
\end{tabular}

\newpage
\begin{align*}
\alpha {\bf x} {\bf y}^T + A \rt A
\end{align*} $\quad$ \newline

\begin{tabular}{|c|c|c|c|c|}
\hline
type of matrix $A$ & float & double & cuComplex & cuDoubleComplex \\ \hline
any $m \times n$ matrix  & cublasSger() & cublasDger() & cublasCgeru() & cublasZgeru() \\ \hline
\end{tabular} $\quad$ \newline \newline \newline

\begin{align*}
\alpha {\bf x} {\bf y}^H + A \rt A
\end{align*} $\quad$ \newline

\begin{tabular}{|c|c|c|c|c|}
\hline
type of matrix $A$ & cuComplex & cuDoubleComplex \\ \hline
any $m \times n$ matrix & cublasCgerc() & cublasZgerc() \\ \hline
\end{tabular}


\newpage


\begin{align*}
\alpha ( {\bf x} {\bf y}^T + {\bf y} {\bf x}^T) + A \rt A
\end{align*} $\quad$ \newline

\begin{tabular}{|c|c|c|c|c|}
\hline
type of matrix $A$ & float & double \\ \hline
{\large symmetric matrix }& cublasSsyr2() & cublasDsyr2() \\ \hline
{\large symmetric matrix in packed format }& cublasSspr2() & cublasDspr2() \\ \hline
\end{tabular} $\quad$ \newline \newline \newline


\begin{align*}
\alpha ( {\bf x} {\bf y}^H + {\bf y} {\bf x}^H) + A \rt A
\end{align*} $\quad$ \newline

\begin{tabular}{|c|c|c|c|c|}
\hline
type of matrix $A$ & cuComplex & cuDoubleComplex \\ \hline
{\large Hermitian }& cublasCher2() & cublasZher2() \\ \hline
{\large Hermitian, packed format }& cublasChpr2() & cublasZhpr2() \\ \hline
\end{tabular}


\newpage
\begin{align*}
\alpha \cdot \text{op}(A) {\bf x} + \beta {\bf y} \rt {\bf y}
\end{align*}
where: 
\begin{displaymath}
   \text{op}(A) = \left\{
     \begin{array}{lr}
       A & : \text{{\tt transa == CUBLAS\_OP\_N}}\\
       A^T & : \text{{\tt transa == CUBLAS\_OP\_T}}\\
       A^H & : \text{{\tt transa == CUBLAS\_OP\_C}}
     \end{array}
   \right.
\end{displaymath} $\quad$ \newline

\begin{tabular}{|p{5cm}|c|c|c|c|}
\hline
type of matrix $A$ & float & double & cuComplex & cuDoubleComplex \\ \hline
{\large any $m \times n$} & cublasSgemv() & cublasDgemv() & cublasCgemv & cublasZgemv()   \\ \hline
{\large $m \times n$, banded} & cublasSgbmv() & cublasDgbmv() & cublasCgbmv & cublasZgbmv()   \\ \hline
{\large symmetric, banded \footnotemark}& cublasSsbmv() & cublasDsbmv() & - &-  \\ \hline
{\large symmetric, packed format ${}^1$}& cublasSspmv() & cublasDspmv() & -& -\\ \hline
{\large symmetric,  lower/upper mode ${}^1$}& cublasSsymv() & cublasDsymv() & -& -\\ \hline
{\large Hermitian ${}^1$}& - & - &  cublasChemv() & cublasZhemv() \\ \hline
{\large Hermitian, banded ${}^1$}& - & - &  cublasChbmv() & cublasZhbmv() \\ \hline
\end{tabular}

\footnotetext{Here, op($A$) = A with no {\tt transa} option.}


\newpage




























\Huge
\begin{center}
{\bf LEVEL 3 FUNCTIONS}
\end{center} $\quad$ \newline \LARGE

\begin{align*}
\alpha \cdot \text{op}(A) \text{op}(B) + \beta C \rt C
\end{align*}
where: 
\begin{displaymath}
   \text{op}(A) = \left\{
     \begin{array}{lr}
       A & : \text{{\tt transa == CUBLAS\_OP\_N}}\\
       A^T & : \text{{\tt transa == CUBLAS\_OP\_T}}\\
       A^H & : \text{{\tt transa == CUBLAS\_OP\_C}}
     \end{array}
   \right.
\end{displaymath} $\quad$ \newline



\begin{tabular}{|c|c|c|c|c|}
\hline
matrices $A$, $B$, $C$ & float & double & cuComplex & cuDoubleComplex \\ \hline
{\large any with compatible sizes} & cublasSgemm() &cublasDgemm() & cublasCgemm() & cublasZgemm() \\ \hline
\end{tabular}
\newpage



Batch of {\tt batchCount} matrices:
\begin{align*}
\alpha \cdot \text{op}(A[i]) \text{op} (B[i]) + \beta C[i] \rt C[i]
\end{align*}

where: 
\begin{displaymath}
   \text{op}(A) = \left\{
     \begin{array}{lr}
       A & : \text{{\tt transa == CUBLAS\_OP\_N}}\\
       A^T & : \text{{\tt transa == CUBLAS\_OP\_T}}\\
       A^H & : \text{{\tt transa == CUBLAS\_OP\_C}}
     \end{array}
   \right.
\end{displaymath} $\quad$ \newline \newline


\begin{tabular}{|c|c|}
\hline
matrices types $A[i]$, $B[i]$, $C[i]$ & any with compatible sizes \\ \hline
float function & cublasSgemmBatched() \\ \hline
double function & cublasDgemmBatched() \\ \hline
cuComplex function & cublasCgemmBatched() \\ \hline
cuDoubleComplex function & cublasZgemmBatched() \\ \hline
\end{tabular}
\newpage


\begin{displaymath}
   \left.
     \begin{array}{lr}
       \alpha A B + \beta C & : \text{{\tt side == CUBLAS\_SIDE\_LEFT }}\\
       \alpha B A + \beta C & : \text{{\tt side == CUBLAS\_SIDE\_RIGHT}}
     \end{array}
   \right \} \rt C
\end{displaymath} $\quad$ \newline \newline



\begin{tabular}{|p{4.5cm}|c|c|c|c|}
\hline
matrices $A$, $B$, $C$ & float & double & cuComplex & cuDoubleComplex \\ \hline
{\large A: symmetric, lower or upper mode} & cublasSsymm() &cublasDsymm() & cublasCsymm() & cublasZsymm() \\ \hline
{\large A: Hermitian, lower or upper mode} &- & - & cublasChemm() & cublasZhemm() \\ \hline
\end{tabular}
\newpage


\begin{displaymath}
   \left.
     \begin{array}{lr}
       \alpha A A^T + \beta C & : \text{{\tt trans == CUBLAS\_OP\_N}}\\
       \alpha A^T A + \beta C & : \text{{\tt trans == CUBLAS\_OP\_T}}
     \end{array}
   \right \} \rt C
\end{displaymath} $\quad$ \newline \newline



\begin{tabular}{|p{4.5cm}|c|c|c|c|}
\hline
matrices $A$, $B$, $C$ & float & double & cuComplex & cuDoubleComplex \\ \hline
{\large C: symmetric, lower or upper mode} & cublasSsyrk() &cublasDsyrk() & cublasCsyrk() & cublasZsyrk() \\ \hline
\end{tabular}

\newpage




\begin{displaymath}
   \left.
     \begin{array}{lr}
       \alpha (A B^T + B A^T)+ \beta C & : \text{{\tt trans == CUBLAS\_OP\_N}}\\
       \alpha(A^T B + B^T A) + \beta C & : \text{{\tt trans == CUBLAS\_OP\_T}}
     \end{array}
   \right \} \rt C
\end{displaymath} $\quad$ \newline \newline



\begin{tabular}{|p{4.5cm}|c|c|c|c|}
\hline
matrices $A$, $B$, $C$ & float & double & cuComplex & cuDoubleComplex \\ \hline
{\large C: symmetric, lower or upper mode} & cublasSsyr2k() &cublasDsyr2k() & cublasCsyr2k() & cublasZsyr2k() \\ \hline
\end{tabular}
\newpage


\begin{displaymath}
   \left.
     \begin{array}{lr}
       \alpha \text{op}(A)B & : \text{{\tt trans == CUBLAS\_SIDE\_LEFT }}\\
       \alpha B \text{op}(A) & : \text{{\tt trans == CUBLAS\_SIDE\_RIGHT}}
     \end{array}
   \right \} \rt C
\end{displaymath}  

where:

\begin{displaymath}
   \text{op}(A) = \left\{
     \begin{array}{lr}
       A & : \text{{\tt transa == CUBLAS\_OP\_N}}\\
       A^T & : \text{{\tt transa == CUBLAS\_OP\_T}}\\
       A^H & : \text{{\tt transa == CUBLAS\_OP\_C}}
     \end{array}
   \right.
\end{displaymath} $\quad$ \newline


\begin{tabular}{|p{4.5cm}|c|c|c|c|}
\hline
matrices $A$, $B$ & float & double & cuComplex & cuDoubleComplex \\ \hline
{\large A: triangular, lower or upper mode} & cublasStrmm() &cublasDtrmm() & cublasCtrmm() & cublasZtrmm() \\ \hline
\end{tabular}
\newpage


Solve for $X$:
\begin{displaymath}
   \left\{
     \begin{array}{lr}
       \text{op}(A)X = \alpha B& : \text{{\tt trans == CUBLAS\_SIDE\_LEFT }}\\
       X \text{op}(A) = \alpha B& : \text{{\tt trans == CUBLAS\_SIDE\_RIGHT}}
     \end{array}
   \right.
\end{displaymath}  

where:

\begin{displaymath}
   \text{op}(A) = \left\{
     \begin{array}{lr}
       A & : \text{{\tt transa == CUBLAS\_OP\_N}}\\
       A^T & : \text{{\tt transa == CUBLAS\_OP\_T}}\\
       A^H & : \text{{\tt transa == CUBLAS\_OP\_C}}
     \end{array}
   \right.
\end{displaymath} $\quad$ \newline


\begin{tabular}{|p{4.5cm}|c|c|c|c|}
\hline
matrices $A$, $B$, $X$ & float & double & cuComplex & cuDoubleComplex \\ \hline
{\large A: triangular, lower or upper mode} & cublasStrsm() &cublasDtrsm() & cublasCtrsm() & cublasZtrsm() \\ \hline
\end{tabular}
\newpage


\begin{align*}
\alpha \cdot \text{op}(A) \text{op}(A)^H + \beta C \rt C
\end{align*}
where: 
\begin{displaymath}
   \text{op}(A) = \left\{
     \begin{array}{lr}
       A & : \text{{\tt transa == CUBLAS\_OP\_N}}\\
       A^H & : \text{{\tt transa == CUBLAS\_OP\_C}}
     \end{array}
   \right.
\end{displaymath} $\quad$ \newline


\begin{tabular}{|c|c|c|c|c|}
\hline
matrices $A$, $B$, $C$ & float & double & cuComplex & cuDoubleComplex \\ \hline
{\large C: Hermitian, lower or upper mode} & - & - & cublasCherk() & cublasZherk() \\ \hline
\end{tabular}
\newpage


\begin{align*}
\alpha \cdot \text{op}(A) \text{op}(B)^H + \ov{\alpha}\text{op}(B) \text{op}(A)^H + \beta \cdot C \rt C
\end{align*}
where:
\begin{displaymath}
   \text{op}(A) = \left\{
     \begin{array}{lr}
       A & : \text{{\tt transa == CUBLAS\_OP\_N}}\\
       A^H & : \text{{\tt transa == CUBLAS\_OP\_C}}
     \end{array}
   \right.
\end{displaymath} $\quad$ \newline \newline


\begin{tabular}{|c|c|c|c|c|}
\hline
matrices $A$, $B$, $C$ & float & double & cuComplex & cuDoubleComplex \\ \hline
{\large C: Hermitian, lower or upper mode} & - & - & cublasCher2k() & cublasZher2k() \\ \hline
\end{tabular}



\newpage
\Huge
\begin{center}
{\bf \huge simpleCUBLAS: EXAMPLE CUBLAS CODE} 
\end{center} 

\setkeys{Gin}{width=.925\textwidth} \includegraphics[scale=0.25,angle=0]{4} \newpage
\setkeys{Gin}{width=1\textwidth} \includegraphics[scale=0.25,angle=0]{5} \newpage
\setkeys{Gin}{width=1.15\textwidth} \includegraphics[scale=0.25,angle=0]{6} \newpage
\setkeys{Gin}{width=1.15\textwidth} \includegraphics[scale=0.25,angle=0]{7} \newpage
\setkeys{Gin}{width=1.15\textwidth} \includegraphics[scale=0.25,angle=0]{8} \newpage
\setkeys{Gin}{width=1.25\textwidth} \includegraphics[scale=0.25,angle=0]{9} \newpage
\setkeys{Gin}{width=1.15\textwidth} \includegraphics[scale=0.25,angle=0]{10} \newpage


\Huge
\begin{center}
{\bf QUICK REVIEW: IMPLEMENTATION OF MATRICES}
\end{center} $\quad$ \newline \Large

Matrices are implemented as linear arrays of memory. For example, CUBLAS thinks of this memory array: \newline

\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
1 & 1 & 2 & 3 & 5 & 8 & 13 & 21 & 34 & 55 & 89 & 144 \\ \hline
\end{tabular} $\quad$ \newline

as this matrix: \newline

\begin{align*}
\begin{bmatrix}
1 & 2 & 5 & 13 & 34 & 89 \\ 
1 & 3 & 8 & 21 & 55 & 144 \\ 
\end{bmatrix} \qquad \text{or this matrix:} \qquad
\begin{bmatrix}
1 & 5 & 34 \\
1 & 8 & 55 \\
2 & 13 & 89 \\
3 & 21 & 144
\end{bmatrix}
\end{align*}

$\quad$ \newline

depending on the number of rows and columns you specify. \newline

NOTE: CUBLAS indexes matrices in column major format.

\newpage

Let: \newline

 $A = $ \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
1 & 1 & 2 & 3 & 5 & 8 & 13 & 21 & 34 & 55 & 89 & 144 \\ \hline
\end{tabular} $\quad$ \newline
\begin{align*}
B = \begin{bmatrix}
1 & 5 & 34 \\
1 & 8 & 55 \\
2 & 13 & 89 \\
3 & 21 & 144
\end{bmatrix}
\end{align*}

Then:

\begin{align*}
B[\text{row }i, \text{ col }j] = A[j \cdot ld + i]
\end{align*} $\quad$ \newline

Where $ld$ stands for ``lead dimension". For column major order matrices, the lead dimension of a matrix is the number of elements in a column. $\quad$ \newline

For indexing in your code, use a function or macro such as:

\begin{verbatim}
                       #define IDX2F(i, j, ld) j * ld + i 
\end{verbatim} $\quad$ \newline

To go from matrix coordinates to the corresponding memory array index. \footnote{Note: use {\tt \#define IDX2F(i, j, ld) (j - 1) * ld + i-1 } for 1-bases matrix coordinates}




\newpage
\Huge
\begin{center}
{\bf  \huge EXAMPLE2: MORE CUBLAS CODE}
\end{center} $\quad$ \newline


\setkeys{Gin}{width=1.15\textwidth} \includegraphics[scale=0.25,angle=0]{1} \newpage
\setkeys{Gin}{width=1.15\textwidth} \includegraphics[scale=0.25,angle=0]{2} \newpage
\setkeys{Gin}{width=1.15\textwidth} \includegraphics[scale=0.25,angle=0]{3} \newpage

\Huge
\begin{center}
{\bf GPU SERIES MATERIALS}
\end{center} $\quad$ \newline
\huge
These slides, a tentative syllabus for the whole series, and code are available at: \newline

\begin{center}
 https://github.com/wlandau/gpu. 
\end{center} $\quad$ \newline


After logging into you home directory on impact1, type: \newline

\begin{verbatim}
        git clone https://github.com/wlandau/gpu
\end{verbatim} $\quad$ \newline

into the command line to download all the materials.

\newpage

\Huge
\begin{center}
{\bf REFERENCES}
\end{center} $\quad$ \newline \large

"CULA Programmer's Guide". CULA Tools. http://www.culatools.com/cula\_dense\_programmers\_guide/



\end{flushleft}
\end{document}