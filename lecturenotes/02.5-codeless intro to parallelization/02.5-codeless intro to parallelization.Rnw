\documentclass{article}

\hoffset = 0pt
\voffset = 0pt
\footskip = 75pt

\usepackage[landscape]{geometry}
\usepackage{amssymb,amsfonts}
\usepackage[all,arc]{xy}
\usepackage{enumerate}
\usepackage{mathrsfs}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{mathrsfs}
\usepackage[all,arc]{xy}
\usepackage{enumerate}
\usepackage{color}
\usepackage{graphicx}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{subfig}
\usepackage{color}

\providecommand{\beq}{\begin{equation*}}
\providecommand{\eeq}{\end{equation*}}
\providecommand{\bs}{\backslash}
\providecommand{\e}{\varepsilon}
\providecommand{\E}{\ \exists \ }
\providecommand{\all}{\ \forall \ }
\providecommand{\Rt}{arrow}
\providecommand{\rt}{arrow}
\providecommand{\vc}[1]{\boldsymbol{#1}}
\providecommand{\N}{\mathbb{N}}
\providecommand{\Q}{\mathbb{Q}}
\providecommand{\R}{\mathbb{R}}
\providecommand{\C}{\mathbb{C}}
\providecommand{\Z}{\mathbb{Z}}
\providecommand{\Qn}{\mathbb{Q}^n}
\providecommand{\Rn}{\mathbb{R}^n}
\providecommand{\Cn}{\mathbb{C}^n}
\providecommand{\Zn}{\mathbb{Z}^n}
\providecommand{\Qk}{\mathbb{Q}^k}
\providecommand{\Rk}{\mathbb{R}^k}
\providecommand{\Ck}{\mathbb{C}^k}
\providecommand{\Zk}{\mathbb{Z}^k}
\providecommand{\ov}[1]{\overline{#1}}
\providecommand{\lmu}[1]{\lim_{#1 arrow \infty}}
\providecommand{\lmd}[1]{\lim_{#1 arrow -\infty}}
\providecommand{\lm}[2]{\lim_{#1 arrow #2}}
\providecommand{\nv}{{}^{-1}}
\providecommand{\aut}[1]{\text{Aut}{ \ #1}}
\providecommand{\inn}[1]{\text{Inn}{ \ #1}}
\providecommand{\cj}[1]{\overline{#1}}
\providecommand{\wh}[1]{\widehat{#1}}

\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{quest}[thm]{Question}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{defns}[thm]{Definitions}
\newtheorem{con}[thm]{Construction}
\newtheorem{exmp}[thm]{Example}
\newtheorem{exmps}[thm]{Examples}
\newtheorem{notn}[thm]{Notation}

\newtheorem{notns}[thm]{Notations}
\newtheorem{addm}[thm]{Addendum}
\newtheorem{exer}[thm]{Exercise}

\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem{rems}[thm]{Remarks}
\newtheorem{warn}[thm]{Warning}
\newtheorem{sch}[thm]{Scholium}

\makeatletter
\let\c@equation\c@thm
\makeatother
\numberwithin{equation}{section}

\bibliographystyle{plain}

\providecommand{\pset}{1}   %PUT PROBLEM SET NUMBRER HERE
\renewcommand{\labelenumi}{\alph{enumi}.} %controls enumerating style
\renewcommand{\labelenumii}{\roman{enumii}.} 

\setcounter{section}{\pset}


% JBN: My main comment is the way you think about grid and parallel computing. We have talked before about a single grid being created for each kernel call to the GPU. Every function you have here (cascading sum, matrix multiplication, and QR decomposition) should be a single kernel call to the GPU. That is, you want a function sum(a) that computes the sum of a. This is one kernel call and therefore one grid. 

\begin{document}
\begin{flushleft}

\Huge
\begin{center}
$\quad$ \newline \newline \newline \newline \newline
{\bf A CODELESS INTRODUCTION TO GPU PARALLELISM}
\end{center} $\quad$ \newline

\begin{center}
{\Large Will Landau, Prof. Jarad Niemi} 
%JBN: I don't think you need to put Matt's name on here and mine is also optional
\end{center}
\newpage

\Huge
\begin{center}
{\bf OUTLINE}
\end{center} $\quad$ \newline

\begin{enumerate}[A. ]
\item A review of GPU parallelism
\item How to GPU-parallelize the following:
\begin{enumerate}[1. ]
\item vector addition
\item the pairwise (cascading) sum 
\item matrix multiplication
\end{enumerate}
\end{enumerate}
\newpage

\Huge
\begin{center}
{\bf HOW THE CPU AND GPU WORK TOGETHER }
\end{center} 

A GPU can't run a whole computer on its own because it doesn't have access to all the computer's hardware. \newline

In a GPU-capable computer, the CPU is the main processor, and the GPU is an optional hardware add-on. \newline

The CPU uses the GPU like a human would use a hand-held calculator: the CPU does all the main thinking and the GPU does the most cumbersome bits and pieces of number-crunching.
\newpage \huge

\setkeys{Gin}{width=1.1\textwidth} \includegraphics[scale=0.25,angle=0]{picts/Lee4.png} 
\newpage

\Huge
\begin{center}
{\bf GPU PARALLELISM}
\end{center}  \huge
\begin{enumerate}[1. ]
\item The CPU sends a CPU-to-GPU command called a {\bf kernel} to a single GPU core.\newline
\item The GPU core multitasks to execute the command: \newline
\begin{enumerate}[a. ]
\item The GPU makes $B \cdot T$ {\color{blue} copies} of the kernel's code, and then runs all those copies simultaneously. Those parallel copies are called {\bf threads}.  \newline
\item The $B \cdot T$ threads are partitioned into $B$ groups, called {\bf blocks}, of $T$ threads each. \newline
\item The sum total of all the threads from a kernel call is a {\bf grid}. 
\end{enumerate} 
\end{enumerate}

\newpage

\setkeys{Gin}{width=.65\textwidth} \includegraphics[scale=0.25,angle=0]{picts/imng.jpg} \newpage

\newpage

\color{black}

\newpage

\Huge
\begin{center}
{\bf WHEN TO PARALLELIZE}
\end{center} $\quad$ \newline \huge


Calculations you want to parallelize: \newline
\begin{itemize}
\item Highly repetitive floating point arithmetic procedures that can all be done simultaneously.
\end{itemize} $\quad$ \newline
  
Calculations you don't want to parallelize: \newline

\begin{itemize}
\item Inherently sequential calculations, such as recursions.
\item Lengthy control flow: if-then statements, etc. 
\item CPU system routines, such as printing to the console. %JBN: that would be funny
\end{itemize}

\newpage

\Huge
\begin{center}
{\bf EXAMPLES OF EASILY PARALLELIZABLE ALGORITHMS}
\end{center} $\quad$ \newline \huge

Linear algebraic algorithms are particularly amenable to GPU computing because they involve a high volume of simple arithmetic. \newline

I will showcase: \newline

\begin{enumerate}[1. ]
\item vector addition
\item the pairwise (cascading) sum 
\item matrix multiplication
\end{enumerate}

\newpage

\Huge
\begin{center}
{\bf VECTOR ADDITION}
\end{center} $\quad$ \newline \Large

Say I have two vectors:  \newline

\begin{align*}
a = \begin{bmatrix} a_1 \\ a_2 \\ \vdots \\ a_n \end{bmatrix} \quad b =  \begin{bmatrix} b_1 \\ b_2 \\ \vdots \\ b_n \end{bmatrix} 
\end{align*} $\quad$ \newline

I compute their sum, $c = a + b$, by: \newline

\begin{align*}
c = \begin{bmatrix} c_1 \\ c_2 \\ \vdots \\ c_n \end{bmatrix}  =  \begin{bmatrix} a_1 + b_1 \\ a_2 + b_2 \\ \vdots \\ a_n + b_n\end{bmatrix} 
\end{align*}
\newpage

\Huge
\begin{center}
{\bf PARALLELIZING VECTOR ADDITION: METHOD 1 OF 3}
\end{center} \Large
\setkeys{Gin}{width=.9\textwidth} \includegraphics[scale=0.25,angle=0]{picts/vadd1} \newpage

\Huge
\begin{center}
{\bf PARALLELIZING VECTOR ADDITION: METHOD 2 OF 3}
\end{center} \Large
\setkeys{Gin}{width=.9\textwidth} \includegraphics[scale=0.25,angle=0]{picts/vadd2} \newpage

\Huge
\begin{center}
{\bf PARALLELIZING VECTOR ADDITION: METHOD 3 OF 3}
\end{center} \Large
\setkeys{Gin}{width=.9\textwidth} \includegraphics[scale=0.25,angle=0]{picts/vadd3} \newpage


\Huge
\begin{center}
{\bf THE PAIRWISE SUM}
\end{center} $\quad$ \newline

Let's take the pairwise sum of the vector:

\begin{align*}
(5, \ 2, \ -3,\  1,\ 1,\ 8,\ 2,\ 6)
\end{align*} $\quad$ \newline

Using one block of 4 threads. \newpage

\setkeys{Gin}{width=1\textwidth} \includegraphics[scale=0.25,angle=0]{picts/pv1} \newpage
\setkeys{Gin}{width=1\textwidth} \includegraphics[scale=0.25,angle=0]{picts/pv2} \newpage
\setkeys{Gin}{width=1\textwidth} \includegraphics[scale=0.25,angle=0]{picts/pv3} \newpage
\setkeys{Gin}{width=1\textwidth} \includegraphics[scale=0.25,angle=0]{picts/pv} \newpage
\setkeys{Gin}{width=1\textwidth} \includegraphics[scale=0.25,angle=0]{picts/pv5} \newpage
\setkeys{Gin}{width=1\textwidth} \includegraphics[scale=0.25,angle=0]{picts/pv6} \newpage
\setkeys{Gin}{width=1\textwidth} \includegraphics[scale=0.25,angle=0]{picts/pv7} \newpage
\setkeys{Gin}{width=1\textwidth} \includegraphics[scale=0.25,angle=0]{picts/pv8} \newpage
\setkeys{Gin}{width=1\textwidth} \includegraphics[scale=0.25,angle=0]{picts/pv9} \newpage


\Huge
\begin{center}
{\bf AN ASIDE: SYNCHRONIZING THREADS}
\end{center} $\quad$ \newline \Huge

{\bf Synchronization}: Waiting for all parallel tasks to reach a checkpoint before allowing any of them to proceed. \newline


\begin{itemize}
\item Threads from the same block can be synchronized easily.
\item  In general, do not try to synchronize threads from different blocks. It's possible, but extremely inefficient.
\end{itemize}


\newpage

\Huge
\begin{center}
{\bf A RIGOROUS DESCRIPTION OF THE PAIRWISE SUM}
\end{center} \LARGE


Suppose you have a vector $X_0 = (x_{(0,0)}, x_{(0,2)}, \ldots, x_{(0,n-1)})$, where $n = 2^m$ for some $m>0$. \newline

 Compute $\sum_{i = 1}^n x_{(0,i)}$ in the following way:  \newline

\begin{enumerate}[1. ]
\item Create a new vector: $X_1 = ( \underbrace{x_{(0, \ 0)} + x_{(0, \ n/2)}}_{x_{(1, \ 0)}}, \  \underbrace{x_{(0,1)} + x_{(0, \ n/2 + 1)}}_{x_{(1,\ 1)}}, \ \ldots, \underbrace{x_{(0, \ n/2-1)} + x_{(0, \ n - 1)}}_{x_{(1, \ n/2 - 1)}})$

\item Create another new vector: $X_2 = ( \underbrace{x_{(1, \ 0)} + x_{(1, \ n/4)}}_{x_{(2, \ 0)}}, \  \underbrace{x_{(1,\ 1)} + x_{(1, \ n/4 + 1)}}_{x_{(2, \ 1)}}, \ \ldots, \underbrace{x_{(1, n/4-1)} + x_{(1, \ n/2 - 1)}}_{x_{(2, \ n/4 - 1)}})$
\item Continue this process until you get a singleton vector: $X_{m} =  ( \underbrace{x_{(m - 1, 0)} +  x_{(m - 1, 1)}}_{x_{(m, 0)}}  ) $
\end{enumerate} $\quad$ \newline
Notice: $\sum_{i = 1}^n x_{(0,i)} = x_{(m, 0)}$




\newpage
\Huge
\begin{center}
{\bf PARALLELIZING THE PAIRWISE SUM}
\end{center} $\quad$ \newline \huge

Spawn one grid with a single block and $n/2$ threads ($n = 2^m$). Starting with $i = 1$, do the following: \newline

\begin{enumerate}[1. ]
\item Set \text{offset} = $n/2^i$. 
\item Assign thread $j$ to compute: 
\begin{align*}
x_{(i,j)} = x_{(i-1, \ j)} + x_{(i-1,\  j + \text{offset})}
\end{align*} for $j = 0, 2, \cdots, \text{offset} - 1$.
\item Wait until all the above $\frac{n}{2^i}$ threads have completed step 2 (i.e., {\bf synchronize} the threads).
\item Integer divide offset by 2. Return to step 2 if offset $> 0$.  
\end{enumerate}


%JBN:Perhaps compare this version of the algorithm to the sequential version.
%       The sequential version has n-1 operations whereas the cascading sum has 
%       n(1/2+1/4+1/8+...+1/2^m) \to n as n\to \infty and yet the cascading sum will be much quicker
%       due to parallelism particularly of the initial steps


\newpage
\huge
\begin{center}
{\bf \LARGE ASIDE: COMPARE TO THE SEQUENTIAL VERSION}
\end{center} 

\setkeys{Gin}{width=.9\textwidth} \includegraphics[scale=0.25,angle=0]{picts/sequential} 

The pairwise sum requires only $\log_2(n)$ sequential steps, whereas the sequential sum requires $n - 1$ steps.

\newpage

\Huge
\begin{center}
{\bf 3. MATRIX MULTIPLICATION}
\end{center} \Large


Consider an $m \times n$ matrix, $A = (a_{ij})$ , and an $n \times p$ matrix, $B = (b_{ij})$. Compute $A \cdot B$: \newline
%JBN: I also suggest assigning $C=AB$ with elements $c_{ij} = a_{i\cdot}b_{\cdot j}$. 
\begin{enumerate}[1. ]
\item Break apart $A$ into its rows: $A = \begin{bmatrix} a_{1\cdot}  \\ a_{2\cdot} \\ \vdots \\ a_{m\cdot} \end{bmatrix}$, where each $a_{i\cdot} = \begin{bmatrix} a_{i1} & a_{i2} & \cdots & a_{in} \end{bmatrix} $
\item Break apart $B$ into its columns: $B = \begin{bmatrix} b_{\cdot 1} & b_{\cdot 2} & \cdots & b_{\cdot p} \end{bmatrix}$, where each $b_{\cdot j} = \begin{bmatrix} b_{1j} \\ b_{2j} \\ \vdots \\ b_{nj} \end{bmatrix}$
\item Compute $C = A \cdot B$ elementwise, using the usual matrix multiplication rules to find each $a_i \cdot b_j$:
%JBN: no need to have this whole matrix, just show a single element $c_{ij}$ where each of these elements can be computed independently.
\begin{align*}
C &= A \cdot B = \begin{bmatrix} 
(a_{1\cdot} \cdot b_{\cdot 1})  & (a_{1\cdot} \cdot b_{\cdot 2}) & \cdots & (a_{1\cdot } \cdot b_{\cdot p}) \\
(a_{2\cdot} \cdot b_{\cdot 1}) & (a_{2\cdot} \cdot b_{\cdot 2}) & {} & (a_{2\cdot} \cdot b_{\cdot p}) \\
\vdots & { } & \ddots & \vdots \\
(a_{m\cdot} \cdot b_{\cdot 1}) &( a_{m\cdot} \cdot b_{\cdot 2}) & \cdots & (a_{m\cdot } \cdot b_{\cdot p})
\end{bmatrix}
\intertext{i.e.:}
C_{(i, j)} &= a_{i\cdot} \cdot b_{\cdot j}
\end{align*}
\end{enumerate} 


\newpage

\Huge
\begin{center}
{\bf PARALLELIZING MATRIX MULTIPLICATION}
\end{center} $\quad$ \newline \huge
%JBN: I believe, for reasonable size matrices, this is done completely within a single block on a single grid, i.e. thread(i,j) computes element c_{ij}...notice the threads are arranged in a matrix.
Spawn one grid with $m \cdot p$ blocks. Assign block $(i,j)$ to compute $C_{(i,j)} = a_{i \cdot} \cdot b_{\cdot j}$. Within each block: \newline
\begin{enumerate}[1. ]
\item Spawn $n$ threads.
\item Tell the $k$'th thread to compute $c_{ijk} = a_{ik} b_{kj}$.
\item Synchronize the $n$ threads to make sure we have finished calculating all of $c_{ij1}, c_{ij2}, \ldots, c_{ijn}$ before proceeding.
\item Compute $C_{(i,j)} = \sum_{k = 1}^n c_{ijk}$ as a pairwise sum.
\end{enumerate}

\newpage

\Huge
\begin{center}
{\bf EXAMPLE}
\end{center} \huge

Say I want to compute $A \cdot B$, where: 

\begin{align*}
A = \begin{bmatrix} 1 & 2 \\
-1 & 5 \\
7 & -9 \end{bmatrix} \quad 
B = \begin{bmatrix}
8 & 8 & 7 \\
3 & 5 & 2
\end{bmatrix}
\intertext{which I'm setting up as:} \\
C = A \cdot B = \begin{bmatrix}
\left ( \begin{bmatrix} 1 & 2\end{bmatrix} \cdot \begin{bmatrix} 8 \\ 3\end{bmatrix}  \right ) & \left (  \begin{bmatrix} 1 & 2 \end{bmatrix} \cdot \begin{bmatrix} 8 \\ 5 \end{bmatrix}  \right ) & \left ( \begin{bmatrix} 1 & 2 \end{bmatrix} \cdot \begin{bmatrix} 7 \\ 2\end{bmatrix} \right ) \\ \\
\left ( \begin{bmatrix} -1 & 5\end{bmatrix} \cdot \begin{bmatrix} 8 \\ 3 \end{bmatrix}  \right) & \left (  \begin{bmatrix} -1 & 5 \end{bmatrix} \cdot \begin{bmatrix} 8 \\ 5 \end{bmatrix} \right ) & \left (\begin{bmatrix} -1 & 5 \end{bmatrix} \cdot \begin{bmatrix} 7 \\ 2\end{bmatrix} \right )\\ \\
\left ( \begin{bmatrix} 7 & -9 \end{bmatrix} \cdot \begin{bmatrix} 8 \\ 3 \end{bmatrix}  \right) & \left (  \begin{bmatrix}  7 & -9\end{bmatrix} \cdot \begin{bmatrix} 8 \\ 5 \end{bmatrix} \right ) & \left (\begin{bmatrix} 7 & -9 \end{bmatrix} \cdot \begin{bmatrix} 7 \\ 2\end{bmatrix} \right )\\
\end{bmatrix}
\end{align*}

\newpage


\setkeys{Gin}{width=1.1\textwidth} \includegraphics[scale=0.25,angle=0]{picts/mat} \newline

We don't need to synchronize the blocks because they can do their jobs independently. \newpage

Consider Block (0,0), which computes $\begin{bmatrix} 1 & 2 \end{bmatrix} \cdot \begin{bmatrix} 8 \\ 3 \end{bmatrix}$:



\setkeys{Gin}{width=1.25\textwidth} \includegraphics[scale=0.25,angle=0]{picts/bl1} \newpage
\setkeys{Gin}{width=1.25\textwidth} \includegraphics[scale=0.25,angle=0]{picts/bl2} \newpage
\setkeys{Gin}{width=1.25\textwidth} \includegraphics[scale=0.25,angle=0]{picts/bl3} \newpage


\newpage

\Huge
\begin{center}
{\bf OUTLINE}
\end{center} $\quad$ \newline

\begin{enumerate}[A. ]
\item A review of GPU parallelism
\item How to GPU-parallelize the following:
\begin{enumerate}[1. ]
\item vector addition
\item the pairwise (cascading) sum 
\item matrix multiplication
\end{enumerate}
\end{enumerate}

\newpage

\Large
\begin{center}
{\bf PREVIEW: {\tt skeleton.cu}, A BARE BONES CUDA C WORKFLOW}
\end{center}  $\quad$ \newline \normalsize
\begin{verbatim}
#include <stdio.h> 
#include <stdlib.h> 
#include <cuda.h>
#include <cuda_runtime.h> 

__global__ void some_kernel(...){...}

int main (void){ 
  // Declare all variables.
  ...
  // Dynamically allocate host memory.
  ...
  // Dynamically allocate device memory.
  ...
  // Write to host memory.
  ... 
  // Copy host memory to device memory.
  ...
  // Execute kernel on the device.
  some_kernel<<< num_blocks, num_theads_per_block >>>(...);
  
  // Write device memory back to host memory.
  ...
  // Free dynamically-allocated host memory
  ...
  // Free dynamically-allocated device memory    
  ...
}
\end{verbatim} $\quad$ \newline

\newpage

\Huge
\begin{center}
{\bf MATERIALS}
\end{center} $\quad$ \newline
\huge
These slides, a tentative syllabus for the whole lecture series, and code are available at: \newline

\begin{center}
 https://github.com/wlandau/gpu. 
\end{center} $\quad$ \newline


After logging into you home directory on impact1, type: \newline

\begin{verbatim}
        git clone https://github.com/wlandau/gpu
\end{verbatim} $\quad$ \newline

into the command line to download all the materials.

\newpage
\Huge
\begin{center}
{\bf REFERENCES}
\end{center} $\quad$ \newline

J. Sanders and E. Kandrot. {\it CUDA by Example}. Addison-Wesley, 2010. \newline




\end{flushleft}
\end{document}
