\documentclass{article}

\hoffset = 0pt
\voffset = 0pt
\footskip = 75pt

\usepackage[landscape]{geometry}
\usepackage{amssymb,amsfonts}
\usepackage[all,arc]{xy}
\usepackage{enumerate}
\usepackage{mathrsfs}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{mathrsfs}
\usepackage[all,arc]{xy}
\usepackage{enumerate}
\usepackage{color}
\usepackage{graphicx}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{subfig}
\usepackage{color}
\usepackage{Sweave}
\usepackage{SASnRdisplay}

\providecommand{\beq}{\begin{equation*}}
\providecommand{\eeq}{\end{equation*}}
\providecommand{\bs}{\backslash}
\providecommand{\e}{\varepsilon}
\providecommand{\E}{\ \exists \ }
\providecommand{\all}{\ \forall \ }
\providecommand{\Rt}{arrow}
\providecommand{\rt}{arrow}
\providecommand{\vc}[1]{\boldsymbol{#1}}
\providecommand{\N}{\mathbb{N}}
\providecommand{\Q}{\mathbb{Q}}
\providecommand{\R}{\mathbb{R}}
\providecommand{\C}{\mathbb{C}}
\providecommand{\Z}{\mathbb{Z}}
\providecommand{\Qn}{\mathbb{Q}^n}
\providecommand{\Rn}{\mathbb{R}^n}
\providecommand{\Cn}{\mathbb{C}^n}
\providecommand{\Zn}{\mathbb{Z}^n}
\providecommand{\Qk}{\mathbb{Q}^k}
\providecommand{\Rk}{\mathbb{R}^k}
\providecommand{\Ck}{\mathbb{C}^k}
\providecommand{\Zk}{\mathbb{Z}^k}
\providecommand{\ov}[1]{\overline{#1}}
\providecommand{\lmu}[1]{\lim_{#1 arrow \infty}}
\providecommand{\lmd}[1]{\lim_{#1 arrow -\infty}}
\providecommand{\lm}[2]{\lim_{#1 arrow #2}}
\providecommand{\nv}{{}^{-1}}
\providecommand{\aut}[1]{\text{Aut}{ \ #1}}
\providecommand{\inn}[1]{\text{Inn}{ \ #1}}
\providecommand{\cj}[1]{\overline{#1}}
\providecommand{\wh}[1]{\widehat{#1}}

\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{quest}[thm]{Question}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{defns}[thm]{Definitions}
\newtheorem{con}[thm]{Construction}
\newtheorem{exmp}[thm]{Example}
\newtheorem{exmps}[thm]{Examples}
\newtheorem{notn}[thm]{Notation}

\newtheorem{notns}[thm]{Notations}
\newtheorem{addm}[thm]{Addendum}
\newtheorem{exer}[thm]{Exercise}

\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem{rems}[thm]{Remarks}
\newtheorem{warn}[thm]{Warning}
\newtheorem{sch}[thm]{Scholium}

\makeatletter
\let\c@equation\c@thm
\makeatother
\numberwithin{equation}{section}

\bibliographystyle{plain}

\providecommand{\pset}{1}   %PUT PROBLEM SET NUMBRER HERE
\renewcommand{\labelenumi}{\alph{enumi}.} %controls enumerating style
\renewcommand{\labelenumii}{\roman{enumii}.} 

\setcounter{section}{\pset}


% JBN: My main comment is the way you think about grid and parallel computing. We have talked before about a single grid being created for each kernel call to the GPU. Every function you have here (cascading sum, matrix multiplication, and QR decomposition) should be a single kernel call to the GPU. That is, you want a function sum(a) that computes the sum of a. This is one kernel call and therefore one grid. 

\begin{document}
\begin{flushleft}

\Huge
\begin{center}
$\quad$ \newline \newline \newline \newline \newline
{\bf A CODELESS INTRODUCTION TO PARALLELIZATION}
\end{center} $\quad$ \newline

\begin{center}
{\Large Will Landau, Prof. Jarad Niemi} 
%JBN: I don't think you need to put Matt's name on here and mine is also optional
\end{center}


\newpage

\Huge
\begin{center}
{\bf HOW THE CPU AND GPU WORK TOGETHER }
\end{center}  \huge

A GPU can't run a whole computer on its own because it can't do control flow and it doesn't have access to all the system hardware. \newline

In a GPU-capable computer, the CPU is the main processor, and the GPU is an optional hardware add-on. \newline

The CPU is the ``master" of the computer, and it can delegate its highest-throughput parallelizable arithmetic load to the GPU ``minion".  \newline

Another analogy: the CPU uses the GPU in the same way that a human uses a hand-held calculator.
\newpage \huge


\setkeys{Gin}{width=1.1\textwidth} \includegraphics[scale=0.25,angle=0]{picts/Lee4.png} 
\newpage




\Huge
\begin{center}
{\bf GPUS AND PARALLELIZATION}
\end{center} $\quad$ \newline  \Huge

{\bf Parallelization}: Running different calculations simultaneously. {\color{blue} It speeds up calculations dramatically, and GPUs are much better at it than CPUs.} \newline

{\bf Kernel}: An instruction set executed on the GPU. (All others are executed on the CPU.)  \newline

{\color{red} In CUDA C, a kernel is any function prefixed with the keyword, {\tt \_\_global\_\_}. (More on that in a later talk.)}


\newpage


\Huge
\begin{center}
{\bf IMPLEMENTING PARALLELIZATION ON THE GPU IN PRACTICE}
\end{center} $\quad$ \newline 

\begin{enumerate}[1. ]
\item The CPU sends a kernel (instruction set) to the GPU.
\item For every time the CPU sends a kernel, the GPU executes the kernel multiple times simultaneously (in {\bf PARALLEL}). Each such execution of the kernel is called a {\bf thread}.  \newline
\end{enumerate}


\newpage

\Huge
\begin{center}
{\bf ORGANIZATION OF THREADS}
\end{center} $\quad$ \newline

{\bf Grid}: The collection of all the threads that are spawned when the CPU sends a kernel to the GPU. \newline

{\bf Block}: A collection of threads within a grid that share memory.  

\newpage

\setkeys{Gin}{width=.65\textwidth} \includegraphics[scale=0.25,angle=0]{picts/imng.jpg} \newpage

\newpage

\color{red}
\Huge
\begin{center}
{\bf IMPORTANT REMARKS:}
\end{center} $\quad$ \newline

\begin{itemize}
\item With one grid per kernel, GRIDS are executed SEQUENTIALLY.
\item Blocks within the same grid are executed SIMULTANEOUSLY (unless otherwise specified).
\item Threads within the same grid are executed SIMULTANEOUSLY (unless otherwise specified), whether they share a block or not.
\end{itemize}
% JBN: Although this is the way to think about what is going on, it is not technically accurate.
%          Some blocks are executed simultaneously while others have already executed or are waiting to execute.
%          Threads within a grid get executed simultaneously within a warp. 
%          The bottom line here is that this might be the way to thinking about what is going on. Don't count on blocks or threads actually being executed simultaneously.

\color{black}

\newpage
\color{blue}
\Huge
\begin{center}
{\bf NOTE: PARALLELIZATION HAS TWO EQUIVALENT DEFINITIONS}
\end{center} $\quad$ \newline

\begin{enumerate}[1. ]
\item Running different calculations simultaneously. 
\item Breaking up a calculation into grids, then into blocks, and then into threads. 
\end{enumerate} $\quad$ \newline

When I say ``parallelization" in practice, I will most likely be referring to definition 2. 
\color{black}

\newpage

\Huge
\begin{center}
{\bf WHEN TO PARALLELIZE}
\end{center} $\quad$ \newline \huge


Calculations you want to parallelize: \newline
\begin{itemize}
\item Repeated floating point arithmetic procedures that can all be done simultaneously.
\item Anything that can be broken down into or framed as such. 
\end{itemize} $\quad$ \newline
  
Calculations you don't want to parallelize: \newline

\begin{itemize}
\item Inherently sequential calculations, such as recursions.
\item Control flow: if-then statements, etc. 
\item CPU system routines, such as printing to the console. %JBN: that would be funny
\end{itemize}

\newpage

\Huge
\begin{center}
{\bf EXAMPLES OF EASILY PARALLELIZABLE ALGORITHMS}
\end{center} $\quad$ \newline \huge

Linear algebraic algorithms are particularly amenable to GPU computing because they involve a high volume of simple arithmetic. \newline

I will showcase: \newline

\begin{enumerate}[1. ]
\item vector addition
\item the pairwise (cascading) sum %JBN: the vector sum example fits right here and is simpler to understand than the cascading sum
\item matrix multiplication
\item the QR factorization
\end{enumerate}

\newpage

\Huge
\begin{center}
{\bf VECTOR ADDITION}
\end{center} $\quad$ \newline \Large

Say I have two vectors:  \newline

\begin{align*}
a = \begin{bmatrix} a_1 \\ a_2 \\ \vdots \\ a_n \end{bmatrix} \quad b =  \begin{bmatrix} b_1 \\ b_2 \\ \vdots \\ b_n \end{bmatrix} 
\end{align*} $\quad$ \newline

I compute their sum, $c = a + b$, by: \newline

\begin{align*}
c = \begin{bmatrix} c_1 \\ c_2 \\ \vdots \\ c_n \end{bmatrix}  =  \begin{bmatrix} a_1 + b_1 \\ a_2 + b_2 \\ \vdots \\ a_n + b_n\end{bmatrix} 
\end{align*}
\newpage

\Huge
\begin{center}
{\bf PARALLELIZING VECTOR ADDITION: METHOD 1 OF 3}
\end{center} \Large
\setkeys{Gin}{width=.9\textwidth} \includegraphics[scale=0.25,angle=0]{picts/vadd1} \newpage

\Huge
\begin{center}
{\bf PARALLELIZING VECTOR ADDITION: METHOD 2 OF 3}
\end{center} \Large
\setkeys{Gin}{width=.9\textwidth} \includegraphics[scale=0.25,angle=0]{picts/vadd2} \newpage

\Huge
\begin{center}
{\bf PARALLELIZING VECTOR ADDITION: METHOD 3 OF 3}
\end{center} \Large
\setkeys{Gin}{width=.9\textwidth} \includegraphics[scale=0.25,angle=0]{picts/vadd3} \newpage




\newpage
\Huge
\begin{center}
{\bf 2. THE PAIRWISE (CASCADING) SUM}
\end{center}   \LARGE 

\setkeys{Gin}{width=1\textwidth} \includegraphics[scale=0.25,angle=0]{picts/psum2} \newline


\newpage

\Huge
\begin{center}
{\bf A RIGOROUS DESCRIPTION}
\end{center} \LARGE


Suppose you have a vector $X_0 = (x_{(0,1)}, x_{(0,2)}, \ldots, x_{(0,n)})$, where $n = 2^m$ for some $m>0$. \newline

 Compute $\sum_{i = 1}^n x_{(0,i)}$ in the following way:  \newline

\begin{enumerate}[1. ]
\item Create a new vector: $X_1 = ( \underbrace{x_{(0,1)} + x_{(0,2)}}_{x_{(1, 1)}}, \  \underbrace{x_{(0,3)} + x_{(0,4)}}_{x_{(1,2)}}, \ \ldots, \underbrace{x_{(0, n-1)} + x_{(0,n)}}_{x_{(1, n/2)}})$
\item Create another new vector: $X_2 = ( \underbrace{x_{(1,1)} + x_{(1,2)}}_{x_{(2, 1)}}, \  \underbrace{x_{(1,3)} + x_{(1,4)}}_{x_{(2,2)}}, \ \ldots, \underbrace{x_{(1, n/2-1)} + x_{(1,n/2)}}_{x_{(2, n/4)}})$
\item Continue this process until you get a singleton vector: $X_{m} =  ( \underbrace{x_{(m - 1, 1)}, \  x_{(m - 1, 2)}}_{x_{(m, 1)}}  ) $
\end{enumerate} $\quad$ \newline
Notice: $\sum_{i = 1}^n x_{(0,i)} = x_{(m, 1)}$
%JBN:Perhaps compare this version of the algorithm to the sequential version.
%       The sequential version has n-1 operations whereas the cascading sum has 
%       n(1/2+1/4+1/8+...+1/2^m) \to n as n\to \infty and yet the cascading sum will be much quicker
%       due to parallelization particularly of the initial steps


\newpage


\Huge
\begin{center}
{\bf PARALLELIZING THE PAIRWISE SUM}
\end{center} $\quad$ \newline

Spawn one grid with a single block and $n = 2^m$ threads. For each iteration $i = 1, 2, \cdots, m$, do the following: \newline

\begin{enumerate}[1. ]
\item Assign thread $j$ to compute: 
\begin{align*}
x_{(i,j)} = x_{(i-1, \ 2j - 1)} + x_{(i-1,\  2j)}
\end{align*} for $j = 1, 2, \cdots, \frac{n}{2^i}$.
\item Wait until all the above $\frac{n}{2^i}$ threads have completed step 1.
\item Set $i = i + 1$ and repeat. 
\end{enumerate}
\newpage
\Huge
\begin{center}
{\bf EXAMPLE}
\end{center} 

\setkeys{Gin}{width=1\textwidth} \includegraphics[scale=0.25,angle=0]{picts/psumt1} \newpage
\setkeys{Gin}{width=1\textwidth} \includegraphics[scale=0.25,angle=0]{picts/psumt2} \newpage
\setkeys{Gin}{width=1\textwidth} \includegraphics[scale=0.25,angle=0]{picts/psumt3} \newpage
\setkeys{Gin}{width=1\textwidth} \includegraphics[scale=0.25,angle=0]{picts/psumt4} \newpage
\setkeys{Gin}{width=1\textwidth} \includegraphics[scale=0.25,angle=0]{picts/psumt5} \newpage


\Huge
\begin{center}
{\bf \huge COMPARE TO THE SEQUENTIAL VERSION}
\end{center} 

\setkeys{Gin}{width=.9\textwidth} \includegraphics[scale=0.25,angle=0]{picts/sequential} 

The pairwise sum requires only $\log_2(n)$ sequential steps, whereas the sequential sum requires $n - 1$ steps.

\newpage
\Huge
\begin{center}
{\bf AN ASIDE: SYNCHRONIZING THREADS}
\end{center} $\quad$ \newline \Huge

{\bf Synchronization}: Waiting for all parallel tasks to reach a checkpoint before allowing any of those tasks to proceed beyond that checkpoint. \newline


NOTE:

\begin{itemize}
\item Threads from the same block can be synchronized easily.
\item  In general, do not try to synchronize threads from different blocks. It's possible, but extremely inefficient.
\end{itemize}

\newpage

\Huge
\begin{center}
{\bf 3. MATRIX MULTIPLICATION}
\end{center} \Large


Consider an $m \times n$ matrix, $A = (a_{ij})$ , and an $n \times p$ matrix, $B = (b_{ij})$. Compute $A \cdot B$: \newline
%JBN: I also suggest assigning $C=AB$ with elements $c_{ij} = a_{i\cdot}b_{\cdot j}$. 
\begin{enumerate}[1. ]
\item Break apart $A$ into its rows: $A = \begin{bmatrix} a_{1\cdot}  \\ a_{2\cdot} \\ \vdots \\ a_{m\cdot} \end{bmatrix}$, where each $a_{i\cdot} = \begin{bmatrix} a_{i1} & a_{i2} & \cdots & a_{in} \end{bmatrix} $
\item Break apart $B$ into its columns: $B = \begin{bmatrix} b_{\cdot 1} & b_{\cdot 2} & \cdots & b_{\cdot p} \end{bmatrix}$, where each $b_{\cdot j} = \begin{bmatrix} b_{1j} \\ b_{2j} \\ \vdots \\ b_{nj} \end{bmatrix}$
\item Compute $C = A \cdot B$ elementwise, using the usual matrix multiplication rules to find each $a_i \cdot b_j$:
%JBN: no need to have this whole matrix, just show a single element $c_{ij}$ where each of these elements can be computed independently.
\begin{align*}
C &= A \cdot B = \begin{bmatrix} 
(a_{1\cdot} \cdot b_{\cdot 1})  & (a_{1\cdot} \cdot b_{\cdot 2}) & \cdots & (a_{1\cdot } \cdot b_{\cdot p}) \\
(a_{2\cdot} \cdot b_{\cdot 1}) & (a_{2\cdot} \cdot b_{\cdot 2}) & {} & (a_{2\cdot} \cdot b_{\cdot p}) \\
\vdots & { } & \ddots & \vdots \\
(a_{m\cdot} \cdot b_{\cdot 1}) &( a_{m\cdot} \cdot b_{\cdot 2}) & \cdots & (a_{m\cdot } \cdot b_{\cdot p})
\end{bmatrix}
\intertext{i.e.:}
C_{(i, j)} &= a_{i\cdot} \cdot b_{\cdot j}
\end{align*}
\end{enumerate} 


\newpage

\Huge
\begin{center}
{\bf PARALLELIZING MATRIX MULTIPLICATION}
\end{center} $\quad$ \newline \huge
%JBN: I believe, for reasonable size matrices, this is done completely within a single block on a single grid, i.e. thread(i,j) computes element c_{ij}...notice the threads are arranged in a matrix.
Spawn one grid with $m \cdot p$ blocks. Assign block $(i,j)$ to compute $C_{(i,j)} = a_{i \cdot} \cdot b_{\cdot j}$ using the following steps: \newline
\begin{enumerate}[1. ]
\item Spawn $n$ threads.
\item Tell the $k$'th thread to compute $c_{ijk} = a_{ik} b_{kj}$.
\item Synchronize the $n$ threads to make sure we have finished calculating all of $c_{ij1}, c_{ij2}, \ldots, c_{ijn}$ before proceeding.
\item Compute $C_{(i,j)} = \sum_{k = 1}^n c_{ijk}$ as a pairwise sum.
\end{enumerate}

\newpage

\Huge
\begin{center}
{\bf EXAMPLE}
\end{center} \huge

Say I want to compute $A \cdot B$, where: 

\begin{align*}
A = \begin{bmatrix} 1 & 2 \\
-1 & 5 \\
7 & -9 \end{bmatrix} \quad 
B = \begin{bmatrix}
8 & 8 & 7 \\
3 & 5 & 2
\end{bmatrix}
\intertext{which I'm setting up as:} \\
C = A \cdot B = \begin{bmatrix}
\left ( \begin{bmatrix} 1 & 2\end{bmatrix} \cdot \begin{bmatrix} 8 \\ 3\end{bmatrix}  \right ) & \left (  \begin{bmatrix} 1 & 2 \end{bmatrix} \cdot \begin{bmatrix} 8 \\ 5 \end{bmatrix}  \right ) & \left ( \begin{bmatrix} 1 & 2 \end{bmatrix} \cdot \begin{bmatrix} 7 \\ 2\end{bmatrix} \right ) \\ \\
\left ( \begin{bmatrix} -1 & 5\end{bmatrix} \cdot \begin{bmatrix} 8 \\ 3 \end{bmatrix}  \right) & \left (  \begin{bmatrix} -1 & 5 \end{bmatrix} \cdot \begin{bmatrix} 8 \\ 5 \end{bmatrix} \right ) & \left (\begin{bmatrix} -1 & 5 \end{bmatrix} \cdot \begin{bmatrix} 7 \\ 2\end{bmatrix} \right )\\ \\
\left ( \begin{bmatrix} 7 & -9 \end{bmatrix} \cdot \begin{bmatrix} 8 \\ 3 \end{bmatrix}  \right) & \left (  \begin{bmatrix}  7 & -9\end{bmatrix} \cdot \begin{bmatrix} 8 \\ 5 \end{bmatrix} \right ) & \left (\begin{bmatrix} 7 & -9 \end{bmatrix} \cdot \begin{bmatrix} 7 \\ 2\end{bmatrix} \right )\\
\end{bmatrix}
\end{align*}

\newpage


\setkeys{Gin}{width=1.1\textwidth} \includegraphics[scale=0.25,angle=0]{picts/mat} \newline

We don't need to synchronize the blocks because we can compute them independently. \newpage

Consider Block (1,1), which computes $\begin{bmatrix} 1 & 2 \end{bmatrix} \cdot \begin{bmatrix} 8 \\ 3 \end{bmatrix}$:



\setkeys{Gin}{width=1.25\textwidth} \includegraphics[scale=0.25,angle=0]{picts/bl1} \newpage
\setkeys{Gin}{width=1.25\textwidth} \includegraphics[scale=0.25,angle=0]{picts/bl2} \newpage
\setkeys{Gin}{width=1.25\textwidth} \includegraphics[scale=0.25,angle=0]{picts/bl3} \newpage

\Huge
\begin{center}
{\bf REFERENCES}
\end{center} $\quad$ \newline

Lay, David C. {\emph Linear Algebra and Its Applications}. 3rd Ed. Addison Wesley, 2006. \newline

J. Sanders and E. Kandrot. {\it CUDA by Example}. Addison-Wesley, 2010. \newline




\end{flushleft}
\end{document}
