\documentclass{article}

\hoffset = 0pt
\voffset = 0pt
\footskip = 75pt

\usepackage[landscape]{geometry}
\usepackage{amssymb,amsfonts}
\usepackage[all,arc]{xy}
\usepackage{enumerate}
\usepackage{mathrsfs}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{mathrsfs}
\usepackage[all,arc]{xy}
\usepackage{enumerate}
\usepackage{color}
\usepackage{graphicx}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{subfig}
\usepackage{color}
\usepackage{Sweave}
\usepackage{SASnRdisplay}

\providecommand{\beq}{\begin{equation*}}
\providecommand{\eeq}{\end{equation*}}
\providecommand{\bs}{\backslash}
\providecommand{\e}{\varepsilon}
\providecommand{\E}{\ \exists \ }
\providecommand{\all}{\ \forall \ }
\providecommand{\Rt}{arrow}
\providecommand{\rt}{arrow}
\providecommand{\vc}[1]{\boldsymbol{#1}}
\providecommand{\N}{\mathbb{N}}
\providecommand{\Q}{\mathbb{Q}}
\providecommand{\R}{\mathbb{R}}
\providecommand{\C}{\mathbb{C}}
\providecommand{\Z}{\mathbb{Z}}
\providecommand{\Qn}{\mathbb{Q}^n}
\providecommand{\Rn}{\mathbb{R}^n}
\providecommand{\Cn}{\mathbb{C}^n}
\providecommand{\Zn}{\mathbb{Z}^n}
\providecommand{\Qk}{\mathbb{Q}^k}
\providecommand{\Rk}{\mathbb{R}^k}
\providecommand{\Ck}{\mathbb{C}^k}
\providecommand{\Zk}{\mathbb{Z}^k}
\providecommand{\ov}[1]{\overline{#1}}
\providecommand{\lmu}[1]{\lim_{#1 arrow \infty}}
\providecommand{\lmd}[1]{\lim_{#1 arrow -\infty}}
\providecommand{\lm}[2]{\lim_{#1 arrow #2}}
\providecommand{\nv}{{}^{-1}}
\providecommand{\aut}[1]{\text{Aut}{ \ #1}}
\providecommand{\inn}[1]{\text{Inn}{ \ #1}}
\providecommand{\cj}[1]{\overline{#1}}
\providecommand{\wh}[1]{\widehat{#1}}

\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{quest}[thm]{Question}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{defns}[thm]{Definitions}
\newtheorem{con}[thm]{Construction}
\newtheorem{exmp}[thm]{Example}
\newtheorem{exmps}[thm]{Examples}
\newtheorem{notn}[thm]{Notation}

\newtheorem{notns}[thm]{Notations}
\newtheorem{addm}[thm]{Addendum}
\newtheorem{exer}[thm]{Exercise}

\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem{rems}[thm]{Remarks}
\newtheorem{warn}[thm]{Warning}
\newtheorem{sch}[thm]{Scholium}

\makeatletter
\let\c@equation\c@thm
\makeatother
\numberwithin{equation}{section}

\bibliographystyle{plain}

\providecommand{\pset}{1}   %PUT PROBLEM SET NUMBRER HERE
\renewcommand{\labelenumi}{\alph{enumi}.} %controls enumerating style
\renewcommand{\labelenumii}{\roman{enumii}.} 

\setcounter{section}{\pset}

\begin{document}
\begin{flushleft}

\Huge
\begin{center}
$\quad$ \newline \newline \newline \newline \newline
{\bf A CODELESS INTRODUCTION TO PARALLELLIZATION}
\end{center} $\quad$ \newline

\begin{center}
{\Large Will Landau, Matt Simpson, Prof. Jarad Niemi}
\end{center}


\newpage

\Huge
\begin{center}
{\bf HOW THE CPU AND GPU WORK TOGETHER }
\end{center}  \huge

A GPU can't run a whole computer on its own because it can't do control flow and it doesn't have access to all the system hardware. \newline

In a GPU-capable computer, the CPU is the main processor, and the GPU is an optional hardware add-on. \newline

The CPU is the ``master" of the computer, and it can delegate its highest-throughput parallelizable arithmetic load to the GPU ``minion".  \newline

Another analogy: the CPU uses the GPU in the same way that a human uses a hand-held calculator.
\newpage \huge


\setkeys{Gin}{width=1.1\textwidth} \includegraphics[scale=0.25,angle=0]{picts/Lee4.png} 
\newpage




\Huge
\begin{center}
{\bf GPUS AND PARALLELIZATION}
\end{center} $\quad$ \newline  \Huge

{\bf Parallelization}: Running different calculations simultaneously. {\color{blue} It speeds up calculations dramatically, and GPUS are much better at it than CPUs.} \newline

{\bf Kernel}: An instruction set executed on the GPU. (All others are executed on the CPU.)  \newline

{\color{red} In CUDA C, a kernel is any function prefixed with the keyword, {\tt \_\_global\_\_}. (More on that in a later talk.)}


\newpage


\Huge
\begin{center}
{\bf IMPLEMENTING PARALLELIZATION ON THE GPU IN PRACTICE}
\end{center} $\quad$ \newline 

\begin{enumerate}[1. ]
\item The CPU sends a kernel (instruction set) to the GPU.
\item For every time the CPU sends a kernel, the GPU executes the kernel multiple times simultaneously (in {\bf PARALLEL}). Each such execution of the kernel is called a {\bf thread}.  \newline
\end{enumerate}


\newpage

\Huge
\begin{center}
{\bf ORGANIZATION OF THREADS}
\end{center} $\quad$ \newline

{\bf Grid}; The collection of all the threads that are spawned when the CPU sends a kernel to the GPU. \newline

{\bf Block}: A collection of threads within a grid that share memory quickly and easily.  

\newpage

\setkeys{Gin}{width=.65\textwidth} \includegraphics[scale=0.25,angle=0]{picts/imng.jpg} \newpage

\newpage

\color{red}
\Huge
\begin{center}
{\bf IMPORTANT REMARKS:}
\end{center} $\quad$ \newline

\begin{itemize}
\item With one grid per kernel, GRIDS are executed SEQUENTIALLY.
\item Blocks within the same grid are executed SIMULTANEOUSLY.
\item Threads within the same grid are executed SIMULTANEOUSLY, whether they share a block or not.
\end{itemize}
\color{black}

\newpage
\color{blue}
\Huge
\begin{center}
{\bf NOTE: PARALLELIZATION HAS TWO EQUIVALENT DEFINITIONS}
\end{center} $\quad$ \newline

\begin{enumerate}[1. ]
\item Running different calculations simultaneously. 
\item Breaking up a calculation into grids, then into blocks, and then into threads. 
\end{enumerate} $\quad$ \newline

When I say ``parallelization" in practice, I will most likely be referring to definition 2. 
\color{black}

\newpage

\Huge
\begin{center}
{\bf WHEN TO PARALLELIZE}
\end{center} $\quad$ \newline \huge


Calculations you want to parallelize: \newline
\begin{itemize}
\item Repeated floating point arithmetic procedures that can all be done simultaneously.
\item Anything that can be broken down into or framed as such. 
\end{itemize} $\quad$ \newline
  
Calculations you don't want to parallelize: \newline

\begin{itemize}
\item Inherently sequential calculations, such as recursions.
\item Control flow: if-then statements, etc. 
\item CPU system routines, such as printing to the console. 
\end{itemize}

\newpage

\Huge
\begin{center}
{\bf EXAMPLES OF EASILY PARALLELIZABLE ALGORITHMS}
\end{center} $\quad$ \newline \huge

Linear algebraic algorithms are particularly amenable to GPU computing because they involve a high volume of simple arithmetic. \newline

I will showcase: \newline

\begin{enumerate}[1. ]
\item the pairwise c(ascading) sum
\item matrix multiplication
\item the QR factorization
\end{enumerate}

\newpage

\Huge
\begin{center}
{\bf 1. THE PAIRWISE (CASCADING) SUM}
\end{center}  \LARGE

\setkeys{Gin}{width=1\textwidth} \includegraphics[scale=0.25,angle=0]{picts/psum2} \newline


\newpage

\Huge
\begin{center}
{\bf A RIGOROUS DESCRIPTION}
\end{center} \LARGE


Suppose you have a vector $X_0 = (x_{(0,1)}, x_{(0,2)}, \ldots, x_{(0,n)})$, where $n = 2^m$ for some $m>0$. \newline

 Compute $\sum_{i = 1}^n x_{(0,i)}$ in the following way:  \newline

\begin{enumerate}[1. ]
\item Create a new vector: $X_1 = ( \underbrace{x_{(0,1)} + x_{(0,2)}}_{x_{(1, 1)}}, \  \underbrace{x_{(0,3)} + x_{(0,4)}}_{x_{(1,2)}}, \ \ldots, \underbrace{x_{(0, n-1)} + x_{(0,n)}}_{x_{(1, n/2)}})$
\item Create another new vector: $X_2 = ( \underbrace{x_{(1,1)} + x_{(1,2)}}_{x_{(2, 1)}}, \  \underbrace{x_{(1,3)} + x_{(1,4)}}_{x_{(2,2)}}, \ \ldots, \underbrace{x_{(1, n/2-1)} + x_{(1,n/2)}}_{x_{(2, n/4)}})$
\item Continue this process until you get a singleton vector: $X_{\log_2(n)} =  ( \underbrace{x_{(\log_2(n) - 1, 1)}, \  x_{(\log_2(n) - 1, 2)}}_{x_{(\log_2(n), 1)}}  ) $
\end{enumerate} $\quad$ \newline

Notice: $\sum_{i = 1}^n x_{(0,i)} = x_{(\log_2(n), 1)}
$


\newpage


\Huge
\begin{center}
{\bf PARALLELIZING THE PAIRWISE SUM}
\end{center} $\quad$ \newline

Create $\log_2(n)$ grids, each to compute $X_1, X_2, \ldots, X_{\log_2(n)}$, respectively, in sequence. For the $i$'th grid: \newline

\begin{enumerate}[1. ]
\item Spawn one block of $n/2^i$ threads. 
\item Let the $j$'th thread compute the $j$'th element of $X_i$ by pairwise summing the appropriate two elements of $X_{i-1}$.
\end{enumerate}

\newpage
\setkeys{Gin}{width=1.1\textwidth} \includegraphics[scale=0.25,angle=0]{picts/grids} \newpage
\setkeys{Gin}{width=1.1\textwidth} \includegraphics[scale=0.25,angle=0]{picts/blocks} \newpage
\setkeys{Gin}{width=1.1\textwidth} \includegraphics[scale=0.25,angle=0]{picts/threads} 




\newpage
\Huge
\begin{center}
{\bf 2. MATRIX MULTIPLICATION}
\end{center} \Large

Consider an $m \times n$ matrices, $A = (a_{ij})$ , and an $n \times p$ matrix, $B = (b_{ij})$. Compute $A \cdot B$: \newline

\begin{enumerate}[1. ]
\item Break apart $A$ into its rows: $A = \begin{bmatrix} a_{1\#}  \\ a_{2\#} \\ \vdots \\ a_{m\#} \end{bmatrix}$, where each $a_{i\#} = \begin{bmatrix} a_{i1} & a_{i2} & \cdots & a_{in} \end{bmatrix} $
\item Break apart $B$ into its columns: $B = \begin{bmatrix} b_{\#1} & b_{\#2} & \cdots & b_{\#p} \end{bmatrix}$, where each $b_{\#j} = \begin{bmatrix} b_{1j} \\ b_{2j} \\ \vdots \\ b_{nj} \end{bmatrix}$
\item Compute $A \cdot B$ elementwise, using the usual matrix multiplication rules to find each $a_i \cdot b_j$:
\begin{align*}
A \cdot B = \begin{bmatrix} 
(a_{1\#} \cdot b_{\#1})  & (a_{1\#} \cdot b_{\#2}) & \cdots & (a_{1\#} \cdot b_{\#p}) \\
(a_{2\#} \cdot b_{\#1}) & (a_{2\#} \cdot b_{\#2}) & {} & (a_{2\#} \cdot b_{\#p}) \\
\vdots & { } & \ddots & \vdots \\
(a_{m\#} \cdot b_{\#1}) &( a_{m\#} \cdot b_{\#2}) & \cdots & (a_{m\#} \cdot b_{\#p})
\end{bmatrix}
\end{align*}
\end{enumerate} $\quad$ \newline


\newpage

\Huge
\begin{center}
{\bf PARALLELIZING MATRIX MULTIPLICATION}
\end{center} $\quad$ \newline \huge
One approach is to use two sequential grids: \newline
\begin{enumerate}[1. ]
\item Grid 1: spawn $m \cdot p$ blocks. The $(i,j)$'th block does the following:
\begin{enumerate}[a. ]
\item Spawn $n$ threads.
\item Tell the $k$'th thread to compute $c_{ink} = a_{ik} b_{kj}$. 
\end{enumerate}
\item Grid 2: spawn $m \cdot p$ blocks. The $(i,j)$'th block does the following:
\begin{enumerate}[a. ]
\item Compute $(A \cdot B)_{(i, j)} = \sum_{k = 1}^n c_{ijk}$ as a pairwise sum.
\end{enumerate}
\end{enumerate}

\newpage

\Huge
\begin{center}
{\bf EXAMPLE}
\end{center} \huge

Say I want to compute $A \cdot B$, where: 

\begin{align*}
A = \begin{bmatrix} 1 & 2 \\
-1 & 5 \\
7 & -9 \end{bmatrix} \quad 
B = \begin{bmatrix}
8 & 8 & 7 \\
3 & 5 & 2
\end{bmatrix}
\intertext{which I'm setting up as:} \\
A \cdot B = \begin{bmatrix}
\left ( \begin{bmatrix} 1 & 2\end{bmatrix} \cdot \begin{bmatrix} 8 \\ 3\end{bmatrix}  \right ) & \left (  \begin{bmatrix} 1 & 2 \end{bmatrix} \cdot \begin{bmatrix} 8 \\ 5 \end{bmatrix}  \right ) & \left ( \begin{bmatrix} 1 & 2 \end{bmatrix} \cdot \begin{bmatrix} 7 \\ 2\end{bmatrix} \right ) \\ \\
\left ( \begin{bmatrix} -1 & 5\end{bmatrix} \cdot \begin{bmatrix} 8 \\ 3 \end{bmatrix}  \right) & \left (  \begin{bmatrix} -1 & 5 \end{bmatrix} \cdot \begin{bmatrix} 8 \\ 5 \end{bmatrix} \right ) & \left (\begin{bmatrix} -1 & 5 \end{bmatrix} \cdot \begin{bmatrix} 7 \\ 2\end{bmatrix} \right )\\ \\
\left ( \begin{bmatrix} 7 & -9 \end{bmatrix} \cdot \begin{bmatrix} 8 \\ 3 \end{bmatrix}  \right) & \left (  \begin{bmatrix}  7 & -9\end{bmatrix} \cdot \begin{bmatrix} 8 \\ 5 \end{bmatrix} \right ) & \left (\begin{bmatrix} 7 & -9 \end{bmatrix} \cdot \begin{bmatrix} 7 \\ 2\end{bmatrix} \right )\\
\end{bmatrix}
\end{align*}

\newpage \Large

\begin{align*}
\text{\huge Grid 1: } \begin{bmatrix}
\left ( \begin{bmatrix} 1 & 2\end{bmatrix} \cdot \begin{bmatrix} 8 \\ 3\end{bmatrix}  \right ) & \left (  \begin{bmatrix} 1 & 2 \end{bmatrix} \cdot \begin{bmatrix} 8 \\ 5 \end{bmatrix}  \right ) & \left ( \begin{bmatrix} 1 & 2 \end{bmatrix} \cdot \begin{bmatrix} 7 \\ 2\end{bmatrix} \right ) \\ \\
\left ( \begin{bmatrix} -1 & 5\end{bmatrix} \cdot \begin{bmatrix} 8 \\ 3 \end{bmatrix}  \right) & \left (  \begin{bmatrix} -1 & 5 \end{bmatrix} \cdot \begin{bmatrix} 8 \\ 5 \end{bmatrix} \right ) & \left (\begin{bmatrix} -1 & 5 \end{bmatrix} \cdot \begin{bmatrix} 7 \\ 2\end{bmatrix} \right )\\ \\
\left ( \begin{bmatrix} 7 & -9 \end{bmatrix} \cdot \begin{bmatrix} 8 \\ 3 \end{bmatrix}  \right) & \left (  \begin{bmatrix}  7 & -9\end{bmatrix} \cdot \begin{bmatrix} 8 \\ 5 \end{bmatrix} \right ) & \left (\begin{bmatrix} 7 & -9 \end{bmatrix} \cdot \begin{bmatrix} 7 \\ 2\end{bmatrix} \right )\\
\end{bmatrix}
\quad \mapsto \quad \begin{bmatrix}
\begin{bmatrix} 8 \\ 6 \end{bmatrix} & \begin{bmatrix} 8 \\ 10 \end{bmatrix} & \begin{bmatrix} 7 \\ 4 \end{bmatrix} \\ \\
\begin{bmatrix} -8 \\ 15 \end{bmatrix} & \begin{bmatrix} -8 \\ 25 \end{bmatrix} & \begin{bmatrix} -7 \\ 10 \end{bmatrix} \\ \\
\begin{bmatrix} 56 \\ -27 \end{bmatrix} & \begin{bmatrix} 56 \\ -45 \end{bmatrix} & \begin{bmatrix} 49 \\ -18 \end{bmatrix}
\end{bmatrix}
\end{align*} 

\newpage


\setkeys{Gin}{width=1.1\textwidth} \includegraphics[scale=0.25,angle=0]{picts/blockmul} \newpage
\setkeys{Gin}{width=1.1\textwidth} \includegraphics[scale=0.25,angle=0]{picts/threadmul} 


\newpage \Large

\begin{align*}
\text{\huge Grid 2: } \begin{bmatrix}
\begin{bmatrix} 8 \\ 6 \end{bmatrix} & \begin{bmatrix} 8 \\ 10 \end{bmatrix} & \begin{bmatrix} 7 \\ 4 \end{bmatrix} \\ \\
\begin{bmatrix} -8 \\ 15 \end{bmatrix} & \begin{bmatrix} -8 \\ 25 \end{bmatrix} & \begin{bmatrix} -7 \\ 10 \end{bmatrix} \\ \\
\begin{bmatrix} 56 \\ -27 \end{bmatrix} & \begin{bmatrix} 56 \\ -45 \end{bmatrix} & \begin{bmatrix} 49 \\ -18 \end{bmatrix}
\end{bmatrix}
\quad \mapsto \quad 
\begin{bmatrix}
14 & 18 & 11 \\
7 & 17 & 3 \\
29 & 11 & 31
\end{bmatrix}
\end{align*} $\quad$ \newline

\newpage

\setkeys{Gin}{width=1.1\textwidth} \includegraphics[scale=0.25,angle=0]{picts/blockmul2} \newpage
\setkeys{Gin}{width=1.1\textwidth} \includegraphics[scale=0.25,angle=0]{picts/threadmul2} 







\newpage



\Huge
\begin{center}
{\bf 3. THE QR FACTORIZATION}
\end{center} $\quad$ \newline \huge
\emph{ Theorem:} Let $A$ be an $m \times n$ matrix with linearly independent columns. Then: \newline
\begin{align*}
A = QR
\end{align*} $\quad$ \newline
where: \newline

\begin{itemize}
\item $Q$ is an $m \times n $ matrix whose columns form an orthonormal basis for the column space of $A$.
\item $R$ is an $n \times n $ upper triangular (and therefore invertible) matrix with all positive entries on the diagonal.
\end{itemize}

\newpage

\Huge
\begin{center}
{\bf EXAMPLE}
\end{center} $\quad$ \newline \LARGE
\begin{align*}
A  = \begin{bmatrix}
5 & 9 \\
1 & 7 \\
-3 & -5 \\
1 & 5
\end{bmatrix}
\end{align*} $\quad$ \newline
The columns of A are linearly independent. Therefore, $A$ has a QR factorization:

\begin{align*}
&A = QR \\
\intertext{The following choices for $Q$ and $R$ work:} \\
Q = \frac{1}{6} \cdot \begin{bmatrix}
5 & -1 \\
1 & 5 \\
-3 & 1 \\
1 & 3
\end{bmatrix}& \quad \quad
R = \begin{bmatrix}
6 & 12 \\
0 & 6
\end{bmatrix}
\end{align*}


\newpage

\Huge
\begin{center}
{\bf FINDING THE QR FACTORIZATION USING ORTHOGONALITY AND THE GRAHAM SCHMIDT PROCESS}
\end{center} $\quad$ \newline \LARGE
{\bf Orthogonal}: two vectors $u$ and $v$ are orthogonal if $u \bullet v = 0$. \newline

 {\color{red} Note: If $u$ and $v$ are two component vectors in $\R^n$, $u \bullet v = 0$ iff they form a right angle. In Euclidian space, orthogonality is the same as perpendicularity. }\newline

Here ``$\bullet$" denotes the dot product (or inner product) of two component vectors: \newline

If $a = (a_1, a_2, \ldots, a_n)$ and $b = (b_1, b_2,  \ldots, b_n)$, then: \newline
 
\begin{align*}
a \bullet b = (a_1 \cdot b_1, \ a_2 \cdot b_2, \  \ldots, \ a_n \cdot b_n)
\end{align*}
 
\newpage

\Huge
\begin{center}
{\bf THE GRAHAM SCHMIDT PROCESS}
\end{center} \Large

To find the QR factorization of $A = [a_1, \ldots, a_n] $ (where $a_1, \ldots, a_n$ are linearly independent), you first want to find an orthogonal basis for the column space of $A$.  \newline

That's where the Graham Schmidt process comes in. The Graham Schmidt process is the construction one such basis, $\{v_1, v_2, \ldots, v_n\}$, by: 

\begin{align*}
&v_1 = a_1 \\ \\
&v_2 = a_2 - \frac{a_2 \bullet v_1}{v_1 \bullet v_1} \cdot v_1 \\ \\
&v_3 = a_3 - \frac{a_3 \bullet v_1}{v_1 \bullet v_1} \cdot v_1 - \frac{a_3 \bullet v_2}{v_2 \bullet v_2} \cdot v_2 \\ 
&\vdots \\ 
&v_n = a_n - \frac{a_n \bullet v_1}{v_1 \bullet v_1} \cdot v_1 - \frac{a_n \bullet v_2}{v_2 \bullet v_2} \cdot v_2 - \cdots - - \frac{a_{n} \bullet v_{n-1}}{v_{n-1} \bullet v_{n-1}} \cdot v_{n-1}
\end{align*} $\quad$ \newline

Even better, you can get an ortho{\bf normal} basis, $\{u_1, \ u_2, \ldots, \ u_n\}$, by:

\begin{align*}
 \{u_1 = \frac{v_1}{{v_1 \bullet v_1}}, \ u_2 = \frac{v_2}{{v_2 \bullet v_2}}, \ \ldots, \ u_n = \frac{v_n}{{v_n \bullet v_n}}  \}
\end{align*}

\newpage

\Huge
\begin{center}
{\bf REARRANGING THAT LONG LIST OF EQUATIONS...}
\end{center} \Large

\begin{align*}
&a_1 = (a_1 \bullet u_1) \cdot u_1 \\ \\
&a_2 = (a_2 \bullet u_1) \cdot u_1 +  (a_2 \bullet u_2) \cdot u_2 \\ \\
&a_3 =  (a_3 \bullet u_1) \cdot u_1 + (a_3 \bullet u_2) \cdot u_2 + (a_3 \bullet u_3) \cdot u_3 \\ 
&\vdots \\ 
&a_n =  (a_n \bullet u_1) \cdot u_1 + (a_n \bullet u_2) \cdot u_2  + (a_3 \bullet u_3) \cdot u_3 + \cdots + (a_n \bullet u_n) \cdot u_n
\end{align*} $\quad$ \newline
 
 We can put it all in matrix form:
 
 \begin{align*}
A = [a_1, \ldots, a_n] \quad \quad Q = [u_1, \ldots, u_n] \quad \quad R = \begin{bmatrix}
(a_1 \bullet u_1) & (a_2 \bullet u_1) & (a_3 \bullet u_1) & \cdots \\
0 & (a_2 \bullet u_2) & (a_3 \bullet u_2) & \cdots  \\
0 & 0 & (a_3 \bullet u_3) & \cdots\\
\vdots & \vdots & \vdots & \ddots
\end{bmatrix}
\end{align*}
And we're all done.

\newpage

\Huge
\begin{center}
{\bf A SUMMARY OF THE WORKFLOW}
\end{center} \Large

\begin{enumerate}[a. ]
\item Start with $A = [a_1, \ldots, a_n] $ 
\item Compute:
\begin{align*}
&v_1 = a_1 \\ \\
&v_2 = a_2 - \frac{a_2 \bullet v_1}{v_1 \bullet v_1} \cdot v_1 \\ \\
&v_3 = a_3 - \frac{a_3 \bullet v_1}{v_1 \bullet v_1} \cdot v_1 - \frac{a_3 \bullet v_2}{v_2 \bullet v_2} \cdot v_2 \\ 
&\vdots \\ 
&v_n = a_n - \frac{a_n \bullet v_1}{v_1 \bullet v_1} \cdot v_1 - \frac{a_n \bullet v_2}{v_2 \bullet v_2} \cdot v_2 - \cdots - - \frac{a_{n} \bullet v_{n-1}}{v_{n-1} \bullet v_{n-1}} \cdot v_{n-1}
\end{align*}  $\quad$ \newline

\item Compute: $
 \{u_1 = \frac{v_1}{{v_1 \bullet v_1}}, \ u_2 = \frac{v_2}{{v_2 \bullet v_2}}, \ \ldots, \ u_n = \frac{v_n}{{v_n \bullet v_n}}  \}$ \newline

\item Compute:
$R = \begin{bmatrix}
(a_1 \bullet u_1) & (a_2 \bullet u_1) & (a_3 \bullet u_1) & \cdots \\
0 & (a_2 \bullet u_2) & (a_3 \bullet u_2) & \cdots  \\
0 & 0 & (a_3 \bullet u_3) & \cdots\\
\vdots & \vdots & \vdots & \ddots
\end{bmatrix}$


\end{enumerate}

\newpage

\Huge
\begin{center}
{\bf ONE WAY TO PARALLELIZE b}
\end{center} \normalsize \color{blue}
\begin{align*}
&v_1 = a_1 \\ \\
&v_2 = a_2 - \frac{a_2 \bullet v_1}{v_1 \bullet v_1} \cdot v_1 \\ \\
&v_3 = a_3 - \frac{a_3 \bullet v_1}{v_1 \bullet v_1} \cdot v_1 - \frac{a_3 \bullet v_2}{v_2 \bullet v_2} \cdot v_2 \\ 
&\vdots \\ 
&v_n = a_n - \frac{a_n \bullet v_1}{v_1 \bullet v_1} \cdot v_1 - \frac{a_n \bullet v_2}{v_2 \bullet v_2} \cdot v_2 - \cdots - \frac{a_{n} \bullet v_{n-1}}{v_{n-1} \bullet v_{n-1}} \cdot v_{n-1}
\end{align*}  $\quad$ \newline

\LARGE \color{black}

Give each $v_i = a_i - \frac{a_i \bullet v_1}{v_1 \bullet v_1} \cdot v_1 - \frac{a_i \bullet v_2}{v_2 \bullet v_2} \cdot v_2 - \cdots - \frac{a_{i} \bullet v_{i-1}}{v_{i-1} \bullet v_{i-1}} \cdot v_{i-1}$ three SEQUENTIAL grids: \newline
\begin{enumerate}[Grid 1: ]
\item Create a block of threads to calculate each of $v_{i-1} \bullet v_{i-i}$ and $a_i \bullet v_j$ for all $j < i$. The other dot products have already been calculated from the step that calculates $v_{i - 1}$.
\item Create one block of threads, where the $i$'th thread computes $-\frac{a_{i} \bullet v_{i-1}}{v_{i-1} \bullet v_{i-1}} \cdot v_{i-1}$. 
\item Create one final block of threads that performs a cascading sum on the terms, $a_i, \  - \frac{a_i \bullet v_1}{v_1 \bullet v_1} \cdot v_1, \  - \frac{a_i \bullet v_2}{v_2 \bullet v_2} \cdot v_2, \  - \cdots, \  - \frac{a_{i} \bullet v_{i-1}}{v_{i-1} \bullet v_{i-1}} \cdot v_{i-1}$
\end{enumerate}

\newpage

\Huge
\begin{center}
{\bf ONE WAY TO PARALLELIZE c}
\end{center} $\quad$ \newline \LARGE \color{blue}
\begin{align*}
 \{u_1 = \frac{v_1}{{v_1 \bullet v_1}}, \ u_2 = \frac{v_2}{{v_2 \bullet v_2}}, \ \ldots, \ u_n = \frac{v_n}{{v_n \bullet v_n}}  \}\end{align*} \newline \color{black}

Spawn a grid containing one block of threads, where the $i$'th thread computes $\frac{v_i}{{v_i \bullet v_i}}$. All the dot products, $v_i \bullet v_i$, should be available from step b.

\newpage

\Huge
\begin{center}
{\bf ONE WAY TO PARALLELIZE d }
\end{center} $\quad$ \newline  \LARGE \color{blue}

\begin{align*}
R = \begin{bmatrix}
(a_1 \bullet u_1) & (a_2 \bullet u_1) & (a_3 \bullet u_1) & \cdots \\
0 & (a_2 \bullet u_2) & (a_3 \bullet u_2) & \cdots  \\
0 & 0 & (a_3 \bullet u_3) & \cdots\\
\vdots & \vdots & \vdots & \ddots
\end{bmatrix}
\end{align*} $\quad$ \newline \color{black}

Spawn a block of threads, where the ($i,j$)'th thread computes $(a_1 \bullet u_1) = \frac{1}{v_1 \bullet v_1} (a_1 \bullet v_1)$ for each $j < i < n$. Each $v_i \bullet v_i$ should be available from step b.


\newpage
\Huge
\begin{center}
{\bf REFERENCES}
\end{center} $\quad$ \newline

Lay, David C. {\emph Linear Algebra and Its Applications}. 3rd Ed. Addison Wesley, 2006. \newline

J. Sanders and E. Kandrot. {\it CUDA by Example}. Addison-Wesley, 2010. \newline




\end{flushleft}
\end{document}
